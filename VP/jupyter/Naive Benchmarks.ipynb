{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Benchmarks for VirulencePredictor\n",
    "\n",
    "Using different models to train on the Host-Pathogen Virulence Prediction dataset. This would create benchmarks to use in the GCMC model.\n",
    "\n",
    "None of these models take in account the underlying graph structure of the model, and thus don't have very high performance.\n",
    "\n",
    "All the models are traditional Machine Learning Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data from the created features for virus and mice.\n",
    "\n",
    "data_repo = 'data/'\n",
    "\n",
    "with open(data_repo + 'graph_info.pkl') as f:\n",
    "    graph_info = pickle.load(f)\n",
    "num_users, num_items, u_nodes, v_nodes, ratings = graph_info\n",
    "\n",
    "with open(data_repo + 'virus_enc_ae.pkl') as f:\n",
    "    u_features = pickle.load(f)\n",
    "\n",
    "with open(data_repo + 'mouse_enc_pca.pkl') as f:\n",
    "    v_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample is created by concatenating the corresponding\n",
    "# virus and mouse nodes in the graph\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(u_nodes)):\n",
    "    u = u_nodes[i]\n",
    "    v = v_nodes[i]\n",
    "\n",
    "    feat_u = u_features[u]\n",
    "    feat_v = v_features[v]\n",
    "    \n",
    "    X.append(np.concatenate((feat_u, feat_v), axis=0))\n",
    "    y.append(ratings[i])\n",
    "    \n",
    "X = np.vstack(X)\n",
    "y = np.array(y, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((306, 200), (306,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "headers = ['a' + str(i + 1) for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>a6</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "      <th>a9</th>\n",
       "      <th>a10</th>\n",
       "      <th>...</th>\n",
       "      <th>a191</th>\n",
       "      <th>a192</th>\n",
       "      <th>a193</th>\n",
       "      <th>a194</th>\n",
       "      <th>a195</th>\n",
       "      <th>a196</th>\n",
       "      <th>a197</th>\n",
       "      <th>a198</th>\n",
       "      <th>a199</th>\n",
       "      <th>a200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.147011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.536587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.054926</td>\n",
       "      <td>-1.107168</td>\n",
       "      <td>-1.15011</td>\n",
       "      <td>-3.535913</td>\n",
       "      <td>0.053217</td>\n",
       "      <td>-1.155408</td>\n",
       "      <td>-2.041763</td>\n",
       "      <td>-3.035815</td>\n",
       "      <td>-2.366308</td>\n",
       "      <td>-2.81482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.914425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.352468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.054926</td>\n",
       "      <td>-1.107168</td>\n",
       "      <td>-1.15011</td>\n",
       "      <td>-3.535913</td>\n",
       "      <td>0.053217</td>\n",
       "      <td>-1.155408</td>\n",
       "      <td>-2.041763</td>\n",
       "      <td>-3.035815</td>\n",
       "      <td>-2.366308</td>\n",
       "      <td>-2.81482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.604977</td>\n",
       "      <td>37.230022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.131760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.054926</td>\n",
       "      <td>-1.107168</td>\n",
       "      <td>-1.15011</td>\n",
       "      <td>-3.535913</td>\n",
       "      <td>0.053217</td>\n",
       "      <td>-1.155408</td>\n",
       "      <td>-2.041763</td>\n",
       "      <td>-3.035815</td>\n",
       "      <td>-2.366308</td>\n",
       "      <td>-2.81482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.438612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.710747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.054926</td>\n",
       "      <td>-1.107168</td>\n",
       "      <td>-1.15011</td>\n",
       "      <td>-3.535913</td>\n",
       "      <td>0.053217</td>\n",
       "      <td>-1.155408</td>\n",
       "      <td>-2.041763</td>\n",
       "      <td>-3.035815</td>\n",
       "      <td>-2.366308</td>\n",
       "      <td>-2.81482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.606718</td>\n",
       "      <td>37.237728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.136921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.054926</td>\n",
       "      <td>-1.107168</td>\n",
       "      <td>-1.15011</td>\n",
       "      <td>-3.535913</td>\n",
       "      <td>0.053217</td>\n",
       "      <td>-1.155408</td>\n",
       "      <td>-2.041763</td>\n",
       "      <td>-3.035815</td>\n",
       "      <td>-2.366308</td>\n",
       "      <td>-2.81482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    a1   a2         a3         a4   a5         a6   a7   a8   a9  a10  \\\n",
       "0  0.0  0.0   0.000000  25.147011  0.0  22.536587  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0   0.000000  24.914425  0.0  22.352468  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  27.604977  37.230022  0.0  19.131760  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0   0.000000  25.438612  0.0  22.710747  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  27.606718  37.237728  0.0  19.136921  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    ...         a191      a192     a193      a194      a195      a196  \\\n",
       "0   ...    -3.054926 -1.107168 -1.15011 -3.535913  0.053217 -1.155408   \n",
       "1   ...    -3.054926 -1.107168 -1.15011 -3.535913  0.053217 -1.155408   \n",
       "2   ...    -3.054926 -1.107168 -1.15011 -3.535913  0.053217 -1.155408   \n",
       "3   ...    -3.054926 -1.107168 -1.15011 -3.535913  0.053217 -1.155408   \n",
       "4   ...    -3.054926 -1.107168 -1.15011 -3.535913  0.053217 -1.155408   \n",
       "\n",
       "       a197      a198      a199     a200  \n",
       "0 -2.041763 -3.035815 -2.366308 -2.81482  \n",
       "1 -2.041763 -3.035815 -2.366308 -2.81482  \n",
       "2 -2.041763 -3.035815 -2.366308 -2.81482  \n",
       "3 -2.041763 -3.035815 -2.366308 -2.81482  \n",
       "4 -2.041763 -3.035815 -2.366308 -2.81482  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that no NaNs exist\n",
    "assert df.isnull().sum().all() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "df = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>a6</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "      <th>a9</th>\n",
       "      <th>a10</th>\n",
       "      <th>...</th>\n",
       "      <th>a191</th>\n",
       "      <th>a192</th>\n",
       "      <th>a193</th>\n",
       "      <th>a194</th>\n",
       "      <th>a195</th>\n",
       "      <th>a196</th>\n",
       "      <th>a197</th>\n",
       "      <th>a198</th>\n",
       "      <th>a199</th>\n",
       "      <th>a200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.987265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407415</td>\n",
       "      <td>0.312794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.488463</td>\n",
       "      <td>0.301066</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032584</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.979200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407415</td>\n",
       "      <td>0.312794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.488463</td>\n",
       "      <td>0.301066</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032584</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133663</td>\n",
       "      <td>0.999763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.838109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407415</td>\n",
       "      <td>0.312794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.488463</td>\n",
       "      <td>0.301066</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032584</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407415</td>\n",
       "      <td>0.312794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.488463</td>\n",
       "      <td>0.301066</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032584</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133671</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.838335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407415</td>\n",
       "      <td>0.312794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.488463</td>\n",
       "      <td>0.301066</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032584</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    a1   a2        a3        a4   a5        a6   a7   a8   a9  a10    ...     \\\n",
       "0  0.0  0.0  0.000000  0.675290  0.0  0.987265  0.0  0.0  0.0  0.0    ...      \n",
       "1  0.0  0.0  0.000000  0.669044  0.0  0.979200  0.0  0.0  0.0  0.0    ...      \n",
       "2  0.0  0.0  0.133663  0.999763  0.0  0.838109  0.0  0.0  0.0  0.0    ...      \n",
       "3  0.0  0.0  0.000000  0.683120  0.0  0.994895  0.0  0.0  0.0  0.0    ...      \n",
       "4  0.0  0.0  0.133671  0.999970  0.0  0.838335  0.0  0.0  0.0  0.0    ...      \n",
       "\n",
       "   a191      a192      a193  a194      a195      a196      a197  a198  \\\n",
       "0   0.0  0.407415  0.312794   0.0  0.488463  0.301066  0.004493   0.0   \n",
       "1   0.0  0.407415  0.312794   0.0  0.488463  0.301066  0.004493   0.0   \n",
       "2   0.0  0.407415  0.312794   0.0  0.488463  0.301066  0.004493   0.0   \n",
       "3   0.0  0.407415  0.312794   0.0  0.488463  0.301066  0.004493   0.0   \n",
       "4   0.0  0.407415  0.312794   0.0  0.488463  0.301066  0.004493   0.0   \n",
       "\n",
       "       a199      a200  \n",
       "0  0.032584  0.016502  \n",
       "1  0.032584  0.016502  \n",
       "2  0.032584  0.016502  \n",
       "3  0.032584  0.016502  \n",
       "4  0.032584  0.016502  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 7., 7., 5., 7., 7., 7., 7., 7., 7., 7., 7., 7., 6., 6., 6., 5.,\n",
       "       5., 6., 6., 3., 5., 7., 7., 7., 7., 7., 7., 4., 4., 7., 7., 4., 4.,\n",
       "       4., 6., 6., 5., 5., 5., 5., 6., 6., 6., 1., 7., 7., 7., 7., 7., 7.,\n",
       "       7., 7., 6., 5., 5., 4., 3., 3., 6., 3., 2., 2., 6., 6., 7., 7., 7.,\n",
       "       7., 2., 2., 6., 6., 1., 1., 1., 1., 1., 4., 4., 4., 4., 3., 7., 3.,\n",
       "       6., 3., 3., 3., 4., 3., 3., 3., 3., 7., 5., 7., 5., 7., 3., 6., 4.,\n",
       "       2., 2., 7., 7., 7., 1., 1., 6., 4., 6., 5., 4., 4., 5., 6., 4., 4.,\n",
       "       4., 3., 4., 4., 5., 8., 5., 5., 5., 4., 4., 4., 5., 5., 5., 4., 2.,\n",
       "       6., 6., 3., 3., 6., 6., 5., 4., 4., 4., 4., 4., 4., 4., 5., 7., 7.,\n",
       "       6., 6., 6., 6., 6., 6., 3., 5., 5., 5., 7., 6., 6., 3., 3., 3., 4.,\n",
       "       3., 4., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 4., 4., 4., 4., 4., 4., 4.,\n",
       "       4., 4., 4., 4., 4., 4., 4., 2., 2., 2., 5., 4., 4., 4., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 5., 4., 7., 7.,\n",
       "       7., 6., 6., 4., 5., 2., 2., 6., 3., 1., 4., 3., 3., 3., 6., 6., 3.,\n",
       "       3., 3., 3., 3., 4., 7., 7., 7., 6., 7., 4., 6., 5., 7., 7., 7., 6.,\n",
       "       8., 7., 2., 8., 7., 7., 7., 7., 7., 7., 2., 2., 1., 1., 6., 6., 3.,\n",
       "       3., 3., 3., 3., 8., 8., 7., 4., 2., 2., 8., 8., 6., 7., 4., 7., 7.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value of k for k-fold cross validation\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(errors):\n",
    "    errors = -1 * errors\n",
    "    return sum([np.sqrt(x) for x in errors])/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Runs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21291404587.72441"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr = LinearRegression()\n",
    "rmse(cross_val_score(lr, df, y, cv=k, scoring='mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.013118863841572"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KernelRidgeRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "kr = KernelRidge(alpha=0.01, kernel='rbf')\n",
    "rmse(cross_val_score(kr, df, y, cv=k, scoring='mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1270876976112003"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Regression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(gamma=0.1, C=1.0, epsilon=0.2)\n",
    "rmse(cross_val_score(svr, df, y, cv=k, scoring='mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8854563121107113"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boosting (AdaBoost)\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ab = AdaBoostRegressor(n_estimators=100)\n",
    "rmse(cross_val_score(ab, df, y, cv=k, scoring='mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9554725398199864"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boosting (Gradient Tree Boosting)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "                                max_depth=1, random_state=0, loss='ls')\n",
    "rmse(cross_val_score(gb, df, y, cv=k, scoring='mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9554725398199864"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bagging (ER Trees)\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "ert = ExtraTreesRegressor(n_estimators=100, max_depth=None, \n",
    "                           min_samples_split=2, random_state=0)\n",
    "rmse(cross_val_score(gb, df, y, cv=k, scoring='mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9671639451455831"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bagging (Random Forest)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "rmse(cross_val_score(rf, df, y, cv=k, scoring='mean_squared_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epsilon': 1.0, 'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "1.9575814889694443\n"
     ]
    }
   ],
   "source": [
    "# For SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000], 'epsilon': [1.0, 0.1, 1e-2, 1e-3, 1e-4]}]\n",
    "\n",
    "clf = GridSearchCV(SVR(), tuned_parameters, cv=5,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print clf.best_params_\n",
    "print np.sqrt(-1 * min(clf.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "1.8394874645586412\n"
     ]
    }
   ],
   "source": [
    "# For KernelRidge\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'alpha': [1e-2, 1e-3, 1e-4]}]\n",
    "\n",
    "clf = GridSearchCV(KernelRidge(), tuned_parameters, cv=5,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print clf.best_params_\n",
    "print np.sqrt(-1 * min(clf.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'learning_rate': 0.1}\n",
      "1.820472360599537\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'n_estimators': [10, 50, 100, 200, 500], \n",
    "                     'learning_rate': [1.0, 0.1, 0.01, 0.001, 0.0001]}]\n",
    "\n",
    "clf = GridSearchCV(AdaBoostRegressor(), tuned_parameters, cv=5,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print clf.best_params_\n",
    "print np.sqrt(-1 * min(clf.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'learning_rate': 0.1}\n",
      "2.2355813049597066\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'n_estimators': [10, 50, 100, 200, 500], \n",
    "                     'learning_rate': [1.0, 0.1, 0.01, 0.001, 0.0001]}]\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingRegressor(), tuned_parameters, cv=5,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print clf.best_params_\n",
    "print np.sqrt(-1 * min(clf.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 10 d = 1 rmse = 1.8848376584969286\n",
      "e = 10 d = 5 rmse = 1.9146488046927772\n",
      "e = 10 d = 10 rmse = 2.0313739451456927\n",
      "e = 10 d = 15 rmse = 2.0423300364539605\n",
      "e = 10 d = 20 rmse = 2.0936688982279144\n",
      "e = 50 d = 1 rmse = 1.9314043703214583\n",
      "e = 50 d = 5 rmse = 1.9782536527660288\n",
      "e = 50 d = 10 rmse = 2.2681292760336604\n",
      "e = 50 d = 15 rmse = 2.349645039290053\n",
      "e = 50 d = 20 rmse = 2.4541071585753103\n",
      "e = 100 d = 1 rmse = 1.9554725398199864\n",
      "e = 100 d = 5 rmse = 2.036981192513548\n",
      "e = 100 d = 10 rmse = 2.2775284174556902\n",
      "e = 100 d = 15 rmse = 2.3552073382112577\n",
      "e = 100 d = 20 rmse = 2.460136649694367\n",
      "e = 200 d = 1 rmse = 1.9847053958602956\n",
      "e = 200 d = 5 rmse = 2.0551582051065664\n",
      "e = 200 d = 10 rmse = 2.278931573728212\n",
      "e = 200 d = 15 rmse = 2.3552395231633203\n",
      "e = 200 d = 20 rmse = 2.460172104826303\n",
      "e = 500 d = 1 rmse = 2.01103531149264\n",
      "e = 500 d = 5 rmse = 2.0634850118242767\n",
      "e = 500 d = 10 rmse = 2.2789563810317306\n",
      "e = 500 d = 15 rmse = 2.355240683089174\n",
      "e = 500 d = 20 rmse = 2.460172798514755\n"
     ]
    }
   ],
   "source": [
    "est = [10, 50, 100, 200, 500]\n",
    "dep = [1, 5, 10, 15, 20]\n",
    "\n",
    "for e in est:\n",
    "    for d in dep:\n",
    "        gb = GradientBoostingRegressor(n_estimators=e, learning_rate=0.1,\n",
    "                                        max_depth=d, random_state=0, loss='ls')\n",
    "        print \"e =\", e, \"d =\", d, \"rmse =\", rmse(cross_val_score(gb, df, y, cv=k, scoring='mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 4 d = 1 rmse = 1.8699030913958379\n"
     ]
    }
   ],
   "source": [
    "e, d = 4, 1\n",
    "gb = GradientBoostingRegressor(n_estimators=e, learning_rate=0.1,\n",
    "                                max_depth=d, random_state=0, loss='ls')\n",
    "print \"e =\", e, \"d =\", d, \"rmse =\", rmse(cross_val_score(gb, df, y, cv=k, scoring='mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 4 d = 1 s = 2 rmse = 1.9480016390964814\n",
      "e = 4 d = 1 s = 3 rmse = 1.9274873869000095\n",
      "e = 4 d = 1 s = 4 rmse = 1.9486832617399614\n",
      "e = 4 d = 1 s = 5 rmse = 1.9579767382141668\n",
      "e = 4 d = 1 s = 10 rmse = 1.952143173943842\n",
      "e = 4 d = 3 s = 2 rmse = 1.9592925553603997\n",
      "e = 4 d = 3 s = 3 rmse = 1.9685703087179043\n",
      "e = 4 d = 3 s = 4 rmse = 2.006243199236747\n",
      "e = 4 d = 3 s = 5 rmse = 2.000917152730568\n",
      "e = 4 d = 3 s = 10 rmse = 1.9992917275189304\n",
      "e = 4 d = 5 s = 2 rmse = 2.0108196250062043\n",
      "e = 4 d = 5 s = 3 rmse = 1.9935280766417658\n",
      "e = 4 d = 5 s = 4 rmse = 2.011285311550851\n",
      "e = 4 d = 5 s = 5 rmse = 1.9545189377314707\n",
      "e = 4 d = 5 s = 10 rmse = 1.9718289683421528\n",
      "e = 4 d = 7 s = 2 rmse = 2.1067249408007394\n",
      "e = 4 d = 7 s = 3 rmse = 2.0708459826145624\n",
      "e = 4 d = 7 s = 4 rmse = 2.166123750513008\n",
      "e = 4 d = 7 s = 5 rmse = 2.022314580113778\n",
      "e = 4 d = 7 s = 10 rmse = 2.0177875783112973\n",
      "e = 4 d = 9 s = 2 rmse = 2.1716065736237544\n",
      "e = 4 d = 9 s = 3 rmse = 2.2591382396435584\n",
      "e = 4 d = 9 s = 4 rmse = 2.1251165171558677\n",
      "e = 4 d = 9 s = 5 rmse = 2.1764241771262207\n",
      "e = 4 d = 9 s = 10 rmse = 2.048679057143503\n",
      "e = 5 d = 1 s = 2 rmse = 1.9573858920400784\n",
      "e = 5 d = 1 s = 3 rmse = 1.9560675840421076\n",
      "e = 5 d = 1 s = 4 rmse = 1.9558557441048652\n",
      "e = 5 d = 1 s = 5 rmse = 1.9394747630198814\n",
      "e = 5 d = 1 s = 10 rmse = 1.9492374152051668\n",
      "e = 5 d = 3 s = 2 rmse = 1.9598440243903397\n",
      "e = 5 d = 3 s = 3 rmse = 1.9636202060671817\n",
      "e = 5 d = 3 s = 4 rmse = 1.9746860493525922\n",
      "e = 5 d = 3 s = 5 rmse = 1.9699365334167573\n",
      "e = 5 d = 3 s = 10 rmse = 1.9635488604198437\n",
      "e = 5 d = 5 s = 2 rmse = 2.0282641062462856\n",
      "e = 5 d = 5 s = 3 rmse = 1.9519775787173206\n",
      "e = 5 d = 5 s = 4 rmse = 1.9989372123545668\n",
      "e = 5 d = 5 s = 5 rmse = 1.984856878165349\n",
      "e = 5 d = 5 s = 10 rmse = 2.0258887393898464\n",
      "e = 5 d = 7 s = 2 rmse = 2.1306693852950245\n",
      "e = 5 d = 7 s = 3 rmse = 2.051429961061735\n",
      "e = 5 d = 7 s = 4 rmse = 2.002887830147489\n",
      "e = 5 d = 7 s = 5 rmse = 2.0649308467530103\n",
      "e = 5 d = 7 s = 10 rmse = 2.0213468974861994\n",
      "e = 5 d = 9 s = 2 rmse = 2.1268132561406117\n",
      "e = 5 d = 9 s = 3 rmse = 2.1921496885657836\n",
      "e = 5 d = 9 s = 4 rmse = 2.095155102044217\n",
      "e = 5 d = 9 s = 5 rmse = 2.1225942135310767\n",
      "e = 5 d = 9 s = 10 rmse = 2.068941571400381\n",
      "e = 7 d = 1 s = 2 rmse = 1.9521987532587652\n",
      "e = 7 d = 1 s = 3 rmse = 1.9534973478595057\n",
      "e = 7 d = 1 s = 4 rmse = 1.9485383749104426\n",
      "e = 7 d = 1 s = 5 rmse = 1.950965440941896\n",
      "e = 7 d = 1 s = 10 rmse = 1.9543652359299688\n",
      "e = 7 d = 3 s = 2 rmse = 1.9696465324311347\n",
      "e = 7 d = 3 s = 3 rmse = 1.9577173985673109\n",
      "e = 7 d = 3 s = 4 rmse = 1.9950336077753523\n",
      "e = 7 d = 3 s = 5 rmse = 1.9403909215863178\n",
      "e = 7 d = 3 s = 10 rmse = 1.9524584685548447\n",
      "e = 7 d = 5 s = 2 rmse = 2.005152589430433\n",
      "e = 7 d = 5 s = 3 rmse = 1.9822352678769057\n",
      "e = 7 d = 5 s = 4 rmse = 2.00696925093538\n",
      "e = 7 d = 5 s = 5 rmse = 1.9951120425128686\n",
      "e = 7 d = 5 s = 10 rmse = 2.0016245277319396\n",
      "e = 7 d = 7 s = 2 rmse = 2.047495786260412\n",
      "e = 7 d = 7 s = 3 rmse = 2.0265883187607323\n",
      "e = 7 d = 7 s = 4 rmse = 2.053263041718992\n",
      "e = 7 d = 7 s = 5 rmse = 2.054718183822432\n",
      "e = 7 d = 7 s = 10 rmse = 2.065755695948738\n",
      "e = 7 d = 9 s = 2 rmse = 2.175275284248675\n",
      "e = 7 d = 9 s = 3 rmse = 2.1429167304285444\n",
      "e = 7 d = 9 s = 4 rmse = 2.07676143090834\n",
      "e = 7 d = 9 s = 5 rmse = 2.065788904950788\n",
      "e = 7 d = 9 s = 10 rmse = 2.024720327405309\n",
      "e = 10 d = 1 s = 2 rmse = 1.9493911791128054\n",
      "e = 10 d = 1 s = 3 rmse = 1.9446928577535605\n",
      "e = 10 d = 1 s = 4 rmse = 1.9486442818849539\n",
      "e = 10 d = 1 s = 5 rmse = 1.9472991474068393\n",
      "e = 10 d = 1 s = 10 rmse = 1.9421275781663638\n",
      "e = 10 d = 3 s = 2 rmse = 1.9744993284423757\n",
      "e = 10 d = 3 s = 3 rmse = 1.951420299330423\n",
      "e = 10 d = 3 s = 4 rmse = 1.9932740085966898\n",
      "e = 10 d = 3 s = 5 rmse = 1.965677778399082\n",
      "e = 10 d = 3 s = 10 rmse = 1.9889484662625\n",
      "e = 10 d = 5 s = 2 rmse = 1.9868397533881552\n",
      "e = 10 d = 5 s = 3 rmse = 2.002441138523906\n",
      "e = 10 d = 5 s = 4 rmse = 1.9882882374826878\n",
      "e = 10 d = 5 s = 5 rmse = 2.001335369935661\n",
      "e = 10 d = 5 s = 10 rmse = 1.956325231547705\n",
      "e = 10 d = 7 s = 2 rmse = 2.086559790967539\n",
      "e = 10 d = 7 s = 3 rmse = 2.078410575330692\n",
      "e = 10 d = 7 s = 4 rmse = 2.022806813446456\n",
      "e = 10 d = 7 s = 5 rmse = 2.0323675063458415\n",
      "e = 10 d = 7 s = 10 rmse = 2.0135674246665607\n",
      "e = 10 d = 9 s = 2 rmse = 2.149249968535936\n",
      "e = 10 d = 9 s = 3 rmse = 2.1437309613294433\n",
      "e = 10 d = 9 s = 4 rmse = 2.1094392855261193\n",
      "e = 10 d = 9 s = 5 rmse = 2.1111924412966823\n",
      "e = 10 d = 9 s = 10 rmse = 2.0231530331741694\n",
      "e = 15 d = 1 s = 2 rmse = 1.9482510323837186\n",
      "e = 15 d = 1 s = 3 rmse = 1.9476867663103945\n",
      "e = 15 d = 1 s = 4 rmse = 1.949847827199854\n",
      "e = 15 d = 1 s = 5 rmse = 1.9430058215174248\n",
      "e = 15 d = 1 s = 10 rmse = 1.9536070523930404\n",
      "e = 15 d = 3 s = 2 rmse = 1.9658573363861875\n",
      "e = 15 d = 3 s = 3 rmse = 1.9761439503769485\n",
      "e = 15 d = 3 s = 4 rmse = 1.9701157207282773\n",
      "e = 15 d = 3 s = 5 rmse = 1.9616279766976668\n",
      "e = 15 d = 3 s = 10 rmse = 1.9594281947050067\n",
      "e = 15 d = 5 s = 2 rmse = 2.000204694251408\n",
      "e = 15 d = 5 s = 3 rmse = 1.9776864917525114\n",
      "e = 15 d = 5 s = 4 rmse = 1.9716835622172941\n",
      "e = 15 d = 5 s = 5 rmse = 1.9943157800202242\n",
      "e = 15 d = 5 s = 10 rmse = 1.97653721465077\n",
      "e = 15 d = 7 s = 2 rmse = 2.0710217358600587\n",
      "e = 15 d = 7 s = 3 rmse = 2.0555469904345474\n",
      "e = 15 d = 7 s = 4 rmse = 2.0451126770311854\n",
      "e = 15 d = 7 s = 5 rmse = 2.017049464542054\n",
      "e = 15 d = 7 s = 10 rmse = 1.985057671230971\n",
      "e = 15 d = 9 s = 2 rmse = 2.1266173626243936\n",
      "e = 15 d = 9 s = 3 rmse = 2.1266804724991304\n",
      "e = 15 d = 9 s = 4 rmse = 2.131206752773987\n",
      "e = 15 d = 9 s = 5 rmse = 2.080404393319889\n",
      "e = 15 d = 9 s = 10 rmse = 2.0374036454389635\n",
      "1.9274873869000095\n"
     ]
    }
   ],
   "source": [
    "est = [4, 5, 7, 10, 15]\n",
    "dep = [1, 3, 5, 7, 9]\n",
    "splits = [2, 3, 4, 5, 10]\n",
    "\n",
    "rmses = []\n",
    "for e in est:\n",
    "    for d in dep:\n",
    "        for s in splits:\n",
    "            gb = ExtraTreesRegressor(n_estimators=e, max_depth=d, min_samples_split=s)\n",
    "            r = rmse(cross_val_score(gb, df, y, cv=k, scoring='mean_squared_error'))\n",
    "            print \"e =\", e, \"d =\", d, \"s =\", s, \"rmse =\", r\n",
    "            rmses.append(r)\n",
    "            \n",
    "print min(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 4 d = 1 s = 2 rmse = 1.9027976702805975\n",
      "e = 4 d = 1 s = 3 rmse = 1.9473277028087197\n",
      "e = 4 d = 1 s = 4 rmse = 1.9049136812418492\n",
      "e = 4 d = 1 s = 5 rmse = 1.9410061773072538\n",
      "e = 4 d = 1 s = 10 rmse = 1.9048426839715085\n",
      "e = 4 d = 3 s = 2 rmse = 1.9452139133018278\n",
      "e = 4 d = 3 s = 3 rmse = 2.008215288865953\n",
      "e = 4 d = 3 s = 4 rmse = 1.9629702197681875\n",
      "e = 4 d = 3 s = 5 rmse = 1.9569575935574264\n",
      "e = 4 d = 3 s = 10 rmse = 1.9915896012410859\n",
      "e = 4 d = 5 s = 2 rmse = 1.9630820232092756\n",
      "e = 4 d = 5 s = 3 rmse = 2.019517181420917\n",
      "e = 4 d = 5 s = 4 rmse = 1.9791532627653374\n",
      "e = 4 d = 5 s = 5 rmse = 2.0243659117010986\n",
      "e = 4 d = 5 s = 10 rmse = 2.0183417380750823\n",
      "e = 4 d = 7 s = 2 rmse = 2.051108744523121\n",
      "e = 4 d = 7 s = 3 rmse = 1.9892701526954917\n",
      "e = 4 d = 7 s = 4 rmse = 2.0369864996186218\n",
      "e = 4 d = 7 s = 5 rmse = 2.110893236609951\n",
      "e = 4 d = 7 s = 10 rmse = 2.0323485434940443\n",
      "e = 4 d = 9 s = 2 rmse = 2.0574268572762957\n",
      "e = 4 d = 9 s = 3 rmse = 2.104765130829885\n",
      "e = 4 d = 9 s = 4 rmse = 2.0476076256506675\n",
      "e = 4 d = 9 s = 5 rmse = 1.9985127744747593\n",
      "e = 4 d = 9 s = 10 rmse = 2.03174043994303\n",
      "e = 5 d = 1 s = 2 rmse = 1.8979315805154306\n",
      "e = 5 d = 1 s = 3 rmse = 1.9124606496782302\n",
      "e = 5 d = 1 s = 4 rmse = 1.9262131187147342\n",
      "e = 5 d = 1 s = 5 rmse = 1.9190905675239798\n",
      "e = 5 d = 1 s = 10 rmse = 1.9133996323848712\n",
      "e = 5 d = 3 s = 2 rmse = 1.9503993859236868\n",
      "e = 5 d = 3 s = 3 rmse = 1.9025752415180226\n",
      "e = 5 d = 3 s = 4 rmse = 1.9649471362080604\n",
      "e = 5 d = 3 s = 5 rmse = 2.0169656703401486\n",
      "e = 5 d = 3 s = 10 rmse = 2.042623830685522\n",
      "e = 5 d = 5 s = 2 rmse = 1.9624365692990469\n",
      "e = 5 d = 5 s = 3 rmse = 1.9718264792225333\n",
      "e = 5 d = 5 s = 4 rmse = 2.040853128931809\n",
      "e = 5 d = 5 s = 5 rmse = 1.964041291469212\n",
      "e = 5 d = 5 s = 10 rmse = 1.9898888138013084\n",
      "e = 5 d = 7 s = 2 rmse = 2.062323854392637\n",
      "e = 5 d = 7 s = 3 rmse = 2.0188862608876086\n",
      "e = 5 d = 7 s = 4 rmse = 1.9758986068257518\n",
      "e = 5 d = 7 s = 5 rmse = 2.050265128318723\n",
      "e = 5 d = 7 s = 10 rmse = 2.083699714051838\n",
      "e = 5 d = 9 s = 2 rmse = 2.032577562788254\n",
      "e = 5 d = 9 s = 3 rmse = 2.008605430395082\n",
      "e = 5 d = 9 s = 4 rmse = 1.964841397539938\n",
      "e = 5 d = 9 s = 5 rmse = 1.9852647368079157\n",
      "e = 5 d = 9 s = 10 rmse = 2.064339652612685\n",
      "e = 7 d = 1 s = 2 rmse = 1.9031866207835382\n",
      "e = 7 d = 1 s = 3 rmse = 1.9249368298711438\n",
      "e = 7 d = 1 s = 4 rmse = 1.8982939004123627\n",
      "e = 7 d = 1 s = 5 rmse = 1.889480267986454\n",
      "e = 7 d = 1 s = 10 rmse = 1.906330369745519\n",
      "e = 7 d = 3 s = 2 rmse = 1.946693530375438\n",
      "e = 7 d = 3 s = 3 rmse = 1.9034000995476295\n",
      "e = 7 d = 3 s = 4 rmse = 1.9517967437854047\n",
      "e = 7 d = 3 s = 5 rmse = 1.9814622219395432\n",
      "e = 7 d = 3 s = 10 rmse = 1.932276801803963\n",
      "e = 7 d = 5 s = 2 rmse = 1.961713651193185\n",
      "e = 7 d = 5 s = 3 rmse = 1.9760441597543383\n",
      "e = 7 d = 5 s = 4 rmse = 1.998251333917476\n",
      "e = 7 d = 5 s = 5 rmse = 1.9400181892414323\n",
      "e = 7 d = 5 s = 10 rmse = 1.9921508701019057\n",
      "e = 7 d = 7 s = 2 rmse = 1.9544635397426529\n",
      "e = 7 d = 7 s = 3 rmse = 1.9627845250116192\n",
      "e = 7 d = 7 s = 4 rmse = 2.050646923997024\n",
      "e = 7 d = 7 s = 5 rmse = 2.0003378439225936\n",
      "e = 7 d = 7 s = 10 rmse = 1.9938825435523584\n",
      "e = 7 d = 9 s = 2 rmse = 2.0467197483839854\n",
      "e = 7 d = 9 s = 3 rmse = 1.9695915889124642\n",
      "e = 7 d = 9 s = 4 rmse = 2.047138415326631\n",
      "e = 7 d = 9 s = 5 rmse = 2.0066403788439597\n",
      "e = 7 d = 9 s = 10 rmse = 1.9537327524525874\n",
      "e = 10 d = 1 s = 2 rmse = 1.9055425786467002\n",
      "e = 10 d = 1 s = 3 rmse = 1.904780377037543\n",
      "e = 10 d = 1 s = 4 rmse = 1.8975532338108951\n",
      "e = 10 d = 1 s = 5 rmse = 1.884853509113603\n",
      "e = 10 d = 1 s = 10 rmse = 1.8973728487003085\n",
      "e = 10 d = 3 s = 2 rmse = 1.9217502878258177\n",
      "e = 10 d = 3 s = 3 rmse = 1.968957415069299\n",
      "e = 10 d = 3 s = 4 rmse = 1.9544991796551705\n",
      "e = 10 d = 3 s = 5 rmse = 1.9524378428232363\n",
      "e = 10 d = 3 s = 10 rmse = 1.9607899708920258\n",
      "e = 10 d = 5 s = 2 rmse = 1.920285913248953\n",
      "e = 10 d = 5 s = 3 rmse = 1.925304850885991\n",
      "e = 10 d = 5 s = 4 rmse = 1.9487517484276524\n",
      "e = 10 d = 5 s = 5 rmse = 1.9714233210424417\n",
      "e = 10 d = 5 s = 10 rmse = 1.9855022714833084\n",
      "e = 10 d = 7 s = 2 rmse = 1.9847035293530362\n",
      "e = 10 d = 7 s = 3 rmse = 2.015666321149896\n",
      "e = 10 d = 7 s = 4 rmse = 1.9687754123887504\n",
      "e = 10 d = 7 s = 5 rmse = 2.007853003676801\n",
      "e = 10 d = 7 s = 10 rmse = 1.9423595138867675\n",
      "e = 10 d = 9 s = 2 rmse = 1.960258422093738\n",
      "e = 10 d = 9 s = 3 rmse = 2.070052187825534\n",
      "e = 10 d = 9 s = 4 rmse = 1.951444660526954\n",
      "e = 10 d = 9 s = 5 rmse = 2.0230531209207108\n",
      "e = 10 d = 9 s = 10 rmse = 1.9694800629398634\n",
      "e = 15 d = 1 s = 2 rmse = 1.9099879831911017\n",
      "e = 15 d = 1 s = 3 rmse = 1.885589697113597\n",
      "e = 15 d = 1 s = 4 rmse = 1.8747160284036368\n",
      "e = 15 d = 1 s = 5 rmse = 1.9070304691394617\n",
      "e = 15 d = 1 s = 10 rmse = 1.9038430019082422\n",
      "e = 15 d = 3 s = 2 rmse = 1.9604712451116444\n",
      "e = 15 d = 3 s = 3 rmse = 1.9378121684925311\n",
      "e = 15 d = 3 s = 4 rmse = 1.9292368849066848\n",
      "e = 15 d = 3 s = 5 rmse = 1.9477441626576002\n",
      "e = 15 d = 3 s = 10 rmse = 1.92775325477067\n",
      "e = 15 d = 5 s = 2 rmse = 1.9093126925729393\n",
      "e = 15 d = 5 s = 3 rmse = 1.9559094601191223\n",
      "e = 15 d = 5 s = 4 rmse = 1.9699818234770365\n",
      "e = 15 d = 5 s = 5 rmse = 1.9127211302409148\n",
      "e = 15 d = 5 s = 10 rmse = 1.9093147748670964\n",
      "e = 15 d = 7 s = 2 rmse = 1.9145706534913696\n",
      "e = 15 d = 7 s = 3 rmse = 1.9481709146651198\n",
      "e = 15 d = 7 s = 4 rmse = 1.9887525541001392\n",
      "e = 15 d = 7 s = 5 rmse = 2.0019571279393182\n",
      "e = 15 d = 7 s = 10 rmse = 1.9630385137987791\n",
      "e = 15 d = 9 s = 2 rmse = 1.9402075046408123\n",
      "e = 15 d = 9 s = 3 rmse = 1.9599650948445777\n",
      "e = 15 d = 9 s = 4 rmse = 1.95751572265223\n",
      "e = 15 d = 9 s = 5 rmse = 1.9962174850805898\n",
      "e = 15 d = 9 s = 10 rmse = 1.9610009191833666\n",
      "1.8747160284036368\n"
     ]
    }
   ],
   "source": [
    "est = [4, 5, 7, 10, 15]\n",
    "dep = [1, 3, 5, 7, 9]\n",
    "splits = [2, 3, 4, 5, 10]\n",
    "\n",
    "rmses = []\n",
    "for e in est:\n",
    "    for d in dep:\n",
    "        for s in splits:\n",
    "            rf = RandomForestRegressor(n_estimators=e, max_depth=d, min_samples_split=s)\n",
    "            r = rmse(cross_val_score(rf, df, y, cv=k, scoring='mean_squared_error'))\n",
    "            print \"e =\", e, \"d =\", d, \"s =\", s, \"rmse =\", r\n",
    "            rmses.append(r)\n",
    "            \n",
    "print min(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 62 samples\n",
      "Epoch 1/500\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 4.5252 - val_loss: 4.9916\n",
      "Epoch 2/500\n",
      "244/244 [==============================] - 0s 149us/step - loss: 4.5167 - val_loss: 4.9829\n",
      "Epoch 3/500\n",
      "244/244 [==============================] - 0s 195us/step - loss: 4.5077 - val_loss: 4.9736\n",
      "Epoch 4/500\n",
      "244/244 [==============================] - 0s 170us/step - loss: 4.4977 - val_loss: 4.9606\n",
      "Epoch 5/500\n",
      "244/244 [==============================] - 0s 176us/step - loss: 4.4700 - val_loss: 4.8818\n",
      "Epoch 6/500\n",
      "244/244 [==============================] - 0s 176us/step - loss: 4.1726 - val_loss: 3.9183\n",
      "Epoch 7/500\n",
      "244/244 [==============================] - 0s 194us/step - loss: 2.2182 - val_loss: 2.0739\n",
      "Epoch 8/500\n",
      "244/244 [==============================] - 0s 195us/step - loss: 1.6126 - val_loss: 2.3561\n",
      "Epoch 9/500\n",
      "244/244 [==============================] - 0s 198us/step - loss: 1.5265 - val_loss: 2.1219\n",
      "Epoch 10/500\n",
      "244/244 [==============================] - 0s 162us/step - loss: 1.4763 - val_loss: 2.2932\n",
      "Epoch 11/500\n",
      "244/244 [==============================] - 0s 205us/step - loss: 1.4302 - val_loss: 2.1452\n",
      "Epoch 12/500\n",
      "244/244 [==============================] - 0s 160us/step - loss: 1.3922 - val_loss: 2.2241\n",
      "Epoch 13/500\n",
      "244/244 [==============================] - 0s 202us/step - loss: 1.3807 - val_loss: 2.2040\n",
      "Epoch 14/500\n",
      "244/244 [==============================] - 0s 181us/step - loss: 1.3847 - val_loss: 2.2269\n",
      "Epoch 15/500\n",
      "244/244 [==============================] - 0s 168us/step - loss: 1.3754 - val_loss: 2.3403\n",
      "Epoch 16/500\n",
      "244/244 [==============================] - 0s 195us/step - loss: 1.4027 - val_loss: 2.2248\n",
      "Epoch 17/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.4135 - val_loss: 2.3253\n",
      "Epoch 18/500\n",
      "244/244 [==============================] - 0s 167us/step - loss: 1.4283 - val_loss: 2.2373\n",
      "Epoch 19/500\n",
      "244/244 [==============================] - 0s 186us/step - loss: 1.3908 - val_loss: 2.2863\n",
      "Epoch 20/500\n",
      "244/244 [==============================] - 0s 214us/step - loss: 1.3924 - val_loss: 2.2590\n",
      "Epoch 21/500\n",
      "244/244 [==============================] - 0s 178us/step - loss: 1.3689 - val_loss: 2.3190\n",
      "Epoch 22/500\n",
      "244/244 [==============================] - 0s 183us/step - loss: 1.3677 - val_loss: 2.2376\n",
      "Epoch 23/500\n",
      "244/244 [==============================] - 0s 193us/step - loss: 1.3251 - val_loss: 2.3050\n",
      "Epoch 24/500\n",
      "244/244 [==============================] - 0s 189us/step - loss: 1.4287 - val_loss: 2.2619\n",
      "Epoch 25/500\n",
      "244/244 [==============================] - 0s 214us/step - loss: 1.3330 - val_loss: 2.2995\n",
      "Epoch 26/500\n",
      "244/244 [==============================] - 0s 171us/step - loss: 1.3551 - val_loss: 2.2324\n",
      "Epoch 27/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.3279 - val_loss: 2.2907\n",
      "Epoch 28/500\n",
      "244/244 [==============================] - 0s 171us/step - loss: 1.3334 - val_loss: 2.2473\n",
      "Epoch 29/500\n",
      "244/244 [==============================] - 0s 199us/step - loss: 1.3326 - val_loss: 2.3090\n",
      "Epoch 30/500\n",
      "244/244 [==============================] - 0s 166us/step - loss: 1.3575 - val_loss: 2.2205\n",
      "Epoch 31/500\n",
      "244/244 [==============================] - 0s 160us/step - loss: 1.3519 - val_loss: 2.3104\n",
      "Epoch 32/500\n",
      "244/244 [==============================] - 0s 193us/step - loss: 1.3206 - val_loss: 2.2533\n",
      "Epoch 33/500\n",
      "244/244 [==============================] - 0s 159us/step - loss: 1.3449 - val_loss: 2.2596\n",
      "Epoch 34/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.3906 - val_loss: 2.3037\n",
      "Epoch 35/500\n",
      "244/244 [==============================] - 0s 190us/step - loss: 1.3196 - val_loss: 2.2939\n",
      "Epoch 36/500\n",
      "244/244 [==============================] - 0s 146us/step - loss: 1.3050 - val_loss: 2.3729\n",
      "Epoch 37/500\n",
      "244/244 [==============================] - 0s 190us/step - loss: 1.3550 - val_loss: 2.3196\n",
      "Epoch 38/500\n",
      "244/244 [==============================] - 0s 211us/step - loss: 1.3355 - val_loss: 2.2873\n",
      "Epoch 39/500\n",
      "244/244 [==============================] - 0s 184us/step - loss: 1.3123 - val_loss: 2.3086\n",
      "Epoch 40/500\n",
      "244/244 [==============================] - 0s 200us/step - loss: 1.3072 - val_loss: 2.3055\n",
      "Epoch 41/500\n",
      "244/244 [==============================] - 0s 169us/step - loss: 1.3169 - val_loss: 2.4060\n",
      "Epoch 42/500\n",
      "244/244 [==============================] - 0s 215us/step - loss: 1.3058 - val_loss: 2.2766\n",
      "Epoch 43/500\n",
      "244/244 [==============================] - 0s 181us/step - loss: 1.3043 - val_loss: 2.3136\n",
      "Epoch 44/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.2983 - val_loss: 2.3259\n",
      "Epoch 45/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.2931 - val_loss: 2.2981\n",
      "Epoch 46/500\n",
      "244/244 [==============================] - 0s 170us/step - loss: 1.2984 - val_loss: 2.2963\n",
      "Epoch 47/500\n",
      "244/244 [==============================] - 0s 190us/step - loss: 1.3162 - val_loss: 2.4224\n",
      "Epoch 48/500\n",
      "244/244 [==============================] - 0s 176us/step - loss: 1.2963 - val_loss: 2.2769\n",
      "Epoch 49/500\n",
      "244/244 [==============================] - 0s 182us/step - loss: 1.3253 - val_loss: 2.3946\n",
      "Epoch 50/500\n",
      "244/244 [==============================] - 0s 154us/step - loss: 1.3094 - val_loss: 2.3000\n",
      "Epoch 51/500\n",
      "244/244 [==============================] - 0s 166us/step - loss: 1.2887 - val_loss: 2.3576\n",
      "Epoch 52/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2935 - val_loss: 2.3217\n",
      "Epoch 53/500\n",
      "244/244 [==============================] - 0s 167us/step - loss: 1.3033 - val_loss: 2.2881\n",
      "Epoch 54/500\n",
      "244/244 [==============================] - 0s 167us/step - loss: 1.2973 - val_loss: 2.3352\n",
      "Epoch 55/500\n",
      "244/244 [==============================] - 0s 202us/step - loss: 1.2942 - val_loss: 2.3259\n",
      "Epoch 56/500\n",
      "244/244 [==============================] - 0s 174us/step - loss: 1.3041 - val_loss: 2.3195\n",
      "Epoch 57/500\n",
      "244/244 [==============================] - 0s 164us/step - loss: 1.2908 - val_loss: 2.4068\n",
      "Epoch 58/500\n",
      "244/244 [==============================] - 0s 184us/step - loss: 1.3137 - val_loss: 2.3644\n",
      "Epoch 59/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.3786 - val_loss: 2.2306\n",
      "Epoch 60/500\n",
      "244/244 [==============================] - 0s 234us/step - loss: 1.4282 - val_loss: 2.4537\n",
      "Epoch 61/500\n",
      "244/244 [==============================] - 0s 206us/step - loss: 1.3766 - val_loss: 2.2498\n",
      "Epoch 62/500\n",
      "244/244 [==============================] - 0s 192us/step - loss: 1.3149 - val_loss: 2.3315\n",
      "Epoch 63/500\n",
      "244/244 [==============================] - 0s 166us/step - loss: 1.2971 - val_loss: 2.3012\n",
      "Epoch 64/500\n",
      "244/244 [==============================] - 0s 141us/step - loss: 1.3218 - val_loss: 2.4255\n",
      "Epoch 65/500\n",
      "244/244 [==============================] - 0s 165us/step - loss: 1.3016 - val_loss: 2.3522\n",
      "Epoch 66/500\n",
      "244/244 [==============================] - 0s 139us/step - loss: 1.2950 - val_loss: 2.3069\n",
      "Epoch 67/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.3120 - val_loss: 2.3020\n",
      "Epoch 68/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.3953 - val_loss: 2.4542\n",
      "Epoch 69/500\n",
      "244/244 [==============================] - 0s 127us/step - loss: 1.4540 - val_loss: 2.3288\n",
      "Epoch 70/500\n",
      "244/244 [==============================] - 0s 159us/step - loss: 1.3390 - val_loss: 2.3318\n",
      "Epoch 71/500\n",
      "244/244 [==============================] - 0s 150us/step - loss: 1.3445 - val_loss: 2.3244\n",
      "Epoch 72/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.3236 - val_loss: 2.3518\n",
      "Epoch 73/500\n",
      "244/244 [==============================] - 0s 130us/step - loss: 1.2979 - val_loss: 2.3479\n",
      "Epoch 74/500\n",
      "244/244 [==============================] - 0s 198us/step - loss: 1.2879 - val_loss: 2.3082\n",
      "Epoch 75/500\n",
      "244/244 [==============================] - 0s 137us/step - loss: 1.2755 - val_loss: 2.3834\n",
      "Epoch 76/500\n",
      "244/244 [==============================] - 0s 167us/step - loss: 1.2817 - val_loss: 2.3713\n",
      "Epoch 77/500\n",
      "244/244 [==============================] - 0s 164us/step - loss: 1.2958 - val_loss: 2.3363\n",
      "Epoch 78/500\n",
      "244/244 [==============================] - 0s 125us/step - loss: 1.2787 - val_loss: 2.3615\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 0s 137us/step - loss: 1.2792 - val_loss: 2.3794\n",
      "Epoch 80/500\n",
      "244/244 [==============================] - 0s 137us/step - loss: 1.2663 - val_loss: 2.3566\n",
      "Epoch 81/500\n",
      "244/244 [==============================] - 0s 124us/step - loss: 1.2718 - val_loss: 2.3972\n",
      "Epoch 82/500\n",
      "244/244 [==============================] - 0s 189us/step - loss: 1.2744 - val_loss: 2.4392\n",
      "Epoch 83/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.2929 - val_loss: 2.3003\n",
      "Epoch 84/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.2892 - val_loss: 2.4924\n",
      "Epoch 85/500\n",
      "244/244 [==============================] - 0s 184us/step - loss: 1.3098 - val_loss: 2.2292\n",
      "Epoch 86/500\n",
      "244/244 [==============================] - 0s 165us/step - loss: 1.3993 - val_loss: 2.4799\n",
      "Epoch 87/500\n",
      "244/244 [==============================] - 0s 161us/step - loss: 1.3165 - val_loss: 2.3148\n",
      "Epoch 88/500\n",
      "244/244 [==============================] - 0s 230us/step - loss: 1.2820 - val_loss: 2.3512\n",
      "Epoch 89/500\n",
      "244/244 [==============================] - 0s 226us/step - loss: 1.2731 - val_loss: 2.3233\n",
      "Epoch 90/500\n",
      "244/244 [==============================] - 0s 262us/step - loss: 1.2765 - val_loss: 2.4452\n",
      "Epoch 91/500\n",
      "244/244 [==============================] - 0s 159us/step - loss: 1.2782 - val_loss: 2.3036\n",
      "Epoch 92/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2975 - val_loss: 2.5658\n",
      "Epoch 93/500\n",
      "244/244 [==============================] - 0s 257us/step - loss: 1.3159 - val_loss: 2.3042\n",
      "Epoch 94/500\n",
      "244/244 [==============================] - 0s 168us/step - loss: 1.2970 - val_loss: 2.4618\n",
      "Epoch 95/500\n",
      "244/244 [==============================] - 0s 220us/step - loss: 1.3098 - val_loss: 2.2918\n",
      "Epoch 96/500\n",
      "244/244 [==============================] - 0s 204us/step - loss: 1.3525 - val_loss: 2.3290\n",
      "Epoch 97/500\n",
      "244/244 [==============================] - 0s 191us/step - loss: 1.3120 - val_loss: 2.4610\n",
      "Epoch 98/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2941 - val_loss: 2.2146\n",
      "Epoch 99/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.3769 - val_loss: 2.4451\n",
      "Epoch 100/500\n",
      "244/244 [==============================] - 0s 141us/step - loss: 1.2997 - val_loss: 2.2821\n",
      "Epoch 101/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2787 - val_loss: 2.3600\n",
      "Epoch 102/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.2750 - val_loss: 2.3297\n",
      "Epoch 103/500\n",
      "244/244 [==============================] - 0s 183us/step - loss: 1.2724 - val_loss: 2.3430\n",
      "Epoch 104/500\n",
      "244/244 [==============================] - 0s 123us/step - loss: 1.2895 - val_loss: 2.4304\n",
      "Epoch 105/500\n",
      "244/244 [==============================] - 0s 206us/step - loss: 1.3101 - val_loss: 2.4740\n",
      "Epoch 106/500\n",
      "244/244 [==============================] - 0s 205us/step - loss: 1.2714 - val_loss: 2.3761\n",
      "Epoch 107/500\n",
      "244/244 [==============================] - 0s 218us/step - loss: 1.2701 - val_loss: 2.3648\n",
      "Epoch 108/500\n",
      "244/244 [==============================] - 0s 209us/step - loss: 1.2617 - val_loss: 2.4027\n",
      "Epoch 109/500\n",
      "244/244 [==============================] - 0s 224us/step - loss: 1.2578 - val_loss: 2.3475\n",
      "Epoch 110/500\n",
      "244/244 [==============================] - 0s 223us/step - loss: 1.2719 - val_loss: 2.4308\n",
      "Epoch 111/500\n",
      "244/244 [==============================] - 0s 137us/step - loss: 1.2669 - val_loss: 2.3423\n",
      "Epoch 112/500\n",
      "244/244 [==============================] - 0s 145us/step - loss: 1.3033 - val_loss: 2.3449\n",
      "Epoch 113/500\n",
      "244/244 [==============================] - 0s 160us/step - loss: 1.2675 - val_loss: 2.4214\n",
      "Epoch 114/500\n",
      "244/244 [==============================] - 0s 174us/step - loss: 1.2578 - val_loss: 2.3558\n",
      "Epoch 115/500\n",
      "244/244 [==============================] - 0s 137us/step - loss: 1.2864 - val_loss: 2.5428\n",
      "Epoch 116/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.3325 - val_loss: 2.3230\n",
      "Epoch 117/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2644 - val_loss: 2.4028\n",
      "Epoch 118/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2818 - val_loss: 2.3748\n",
      "Epoch 119/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.2608 - val_loss: 2.3217\n",
      "Epoch 120/500\n",
      "244/244 [==============================] - 0s 147us/step - loss: 1.2781 - val_loss: 2.4744\n",
      "Epoch 121/500\n",
      "244/244 [==============================] - 0s 143us/step - loss: 1.2826 - val_loss: 2.3558\n",
      "Epoch 122/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.2765 - val_loss: 2.3915\n",
      "Epoch 123/500\n",
      "244/244 [==============================] - 0s 154us/step - loss: 1.2895 - val_loss: 2.4115\n",
      "Epoch 124/500\n",
      "244/244 [==============================] - 0s 197us/step - loss: 1.2674 - val_loss: 2.3559\n",
      "Epoch 125/500\n",
      "244/244 [==============================] - 0s 133us/step - loss: 1.2723 - val_loss: 2.4371\n",
      "Epoch 126/500\n",
      "244/244 [==============================] - 0s 186us/step - loss: 1.2587 - val_loss: 2.3233\n",
      "Epoch 127/500\n",
      "244/244 [==============================] - 0s 225us/step - loss: 1.3142 - val_loss: 2.3666\n",
      "Epoch 128/500\n",
      "244/244 [==============================] - 0s 178us/step - loss: 1.2681 - val_loss: 2.4294\n",
      "Epoch 129/500\n",
      "244/244 [==============================] - 0s 224us/step - loss: 1.2608 - val_loss: 2.4108\n",
      "Epoch 130/500\n",
      "244/244 [==============================] - 0s 170us/step - loss: 1.2614 - val_loss: 2.3935\n",
      "Epoch 131/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2603 - val_loss: 2.3729\n",
      "Epoch 132/500\n",
      "244/244 [==============================] - 0s 194us/step - loss: 1.2574 - val_loss: 2.3611\n",
      "Epoch 133/500\n",
      "244/244 [==============================] - 0s 146us/step - loss: 1.2523 - val_loss: 2.4243\n",
      "Epoch 134/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.3033 - val_loss: 2.4643\n",
      "Epoch 135/500\n",
      "244/244 [==============================] - 0s 201us/step - loss: 1.3086 - val_loss: 2.3174\n",
      "Epoch 136/500\n",
      "244/244 [==============================] - 0s 174us/step - loss: 1.3348 - val_loss: 2.5826\n",
      "Epoch 137/500\n",
      "244/244 [==============================] - 0s 180us/step - loss: 1.3436 - val_loss: 2.2688\n",
      "Epoch 138/500\n",
      "244/244 [==============================] - 0s 140us/step - loss: 1.3907 - val_loss: 2.5387\n",
      "Epoch 139/500\n",
      "244/244 [==============================] - 0s 137us/step - loss: 1.3776 - val_loss: 2.2430\n",
      "Epoch 140/500\n",
      "244/244 [==============================] - 0s 148us/step - loss: 1.3007 - val_loss: 2.4667\n",
      "Epoch 141/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.3000 - val_loss: 2.3240\n",
      "Epoch 142/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2732 - val_loss: 2.3627\n",
      "Epoch 143/500\n",
      "244/244 [==============================] - 0s 191us/step - loss: 1.2592 - val_loss: 2.3075\n",
      "Epoch 144/500\n",
      "244/244 [==============================] - 0s 150us/step - loss: 1.2821 - val_loss: 2.4164\n",
      "Epoch 145/500\n",
      "244/244 [==============================] - 0s 175us/step - loss: 1.2709 - val_loss: 2.3637\n",
      "Epoch 146/500\n",
      "244/244 [==============================] - 0s 145us/step - loss: 1.2744 - val_loss: 2.3432\n",
      "Epoch 147/500\n",
      "244/244 [==============================] - 0s 194us/step - loss: 1.3126 - val_loss: 2.4808\n",
      "Epoch 148/500\n",
      "244/244 [==============================] - 0s 166us/step - loss: 1.2964 - val_loss: 2.2964\n",
      "Epoch 149/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.2968 - val_loss: 2.4207\n",
      "Epoch 150/500\n",
      "244/244 [==============================] - 0s 187us/step - loss: 1.2662 - val_loss: 2.3327\n",
      "Epoch 151/500\n",
      "244/244 [==============================] - 0s 212us/step - loss: 1.2543 - val_loss: 2.3741\n",
      "Epoch 152/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.2650 - val_loss: 2.3798\n",
      "Epoch 153/500\n",
      "244/244 [==============================] - 0s 147us/step - loss: 1.2494 - val_loss: 2.3784\n",
      "Epoch 154/500\n",
      "244/244 [==============================] - 0s 148us/step - loss: 1.2523 - val_loss: 2.3577\n",
      "Epoch 155/500\n",
      "244/244 [==============================] - 0s 148us/step - loss: 1.2590 - val_loss: 2.4243\n",
      "Epoch 156/500\n",
      "244/244 [==============================] - 0s 165us/step - loss: 1.2828 - val_loss: 2.3517\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 0s 197us/step - loss: 1.2614 - val_loss: 2.3701\n",
      "Epoch 158/500\n",
      "244/244 [==============================] - 0s 195us/step - loss: 1.2494 - val_loss: 2.3896\n",
      "Epoch 159/500\n",
      "244/244 [==============================] - 0s 236us/step - loss: 1.2482 - val_loss: 2.4494\n",
      "Epoch 160/500\n",
      "244/244 [==============================] - 0s 297us/step - loss: 1.2736 - val_loss: 2.3536\n",
      "Epoch 161/500\n",
      "244/244 [==============================] - 0s 199us/step - loss: 1.2900 - val_loss: 2.3621\n",
      "Epoch 162/500\n",
      "244/244 [==============================] - 0s 257us/step - loss: 1.2881 - val_loss: 2.3885\n",
      "Epoch 163/500\n",
      "244/244 [==============================] - 0s 146us/step - loss: 1.2565 - val_loss: 2.3854\n",
      "Epoch 164/500\n",
      "244/244 [==============================] - ETA: 0s - loss: 1.471 - 0s 161us/step - loss: 1.2522 - val_loss: 2.3984\n",
      "Epoch 165/500\n",
      "244/244 [==============================] - 0s 176us/step - loss: 1.2533 - val_loss: 2.4254\n",
      "Epoch 166/500\n",
      "244/244 [==============================] - 0s 136us/step - loss: 1.2846 - val_loss: 2.3061\n",
      "Epoch 167/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2649 - val_loss: 2.4500\n",
      "Epoch 168/500\n",
      "244/244 [==============================] - 0s 209us/step - loss: 1.2764 - val_loss: 2.3456\n",
      "Epoch 169/500\n",
      "244/244 [==============================] - 0s 136us/step - loss: 1.2730 - val_loss: 2.3680\n",
      "Epoch 170/500\n",
      "244/244 [==============================] - 0s 144us/step - loss: 1.2533 - val_loss: 2.3636\n",
      "Epoch 171/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.2496 - val_loss: 2.3881\n",
      "Epoch 172/500\n",
      "244/244 [==============================] - 0s 171us/step - loss: 1.2533 - val_loss: 2.3615\n",
      "Epoch 173/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2620 - val_loss: 2.4347\n",
      "Epoch 174/500\n",
      "244/244 [==============================] - 0s 164us/step - loss: 1.2549 - val_loss: 2.3117\n",
      "Epoch 175/500\n",
      "244/244 [==============================] - 0s 114us/step - loss: 1.2827 - val_loss: 2.4777\n",
      "Epoch 176/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2745 - val_loss: 2.3339\n",
      "Epoch 177/500\n",
      "244/244 [==============================] - 0s 169us/step - loss: 1.2823 - val_loss: 2.5161\n",
      "Epoch 178/500\n",
      "244/244 [==============================] - 0s 144us/step - loss: 1.2944 - val_loss: 2.3350\n",
      "Epoch 179/500\n",
      "244/244 [==============================] - 0s 191us/step - loss: 1.2741 - val_loss: 2.3391\n",
      "Epoch 180/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.2648 - val_loss: 2.4047\n",
      "Epoch 181/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.2760 - val_loss: 2.3586\n",
      "Epoch 182/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.2725 - val_loss: 2.3508\n",
      "Epoch 183/500\n",
      "244/244 [==============================] - 0s 185us/step - loss: 1.2831 - val_loss: 2.4100\n",
      "Epoch 184/500\n",
      "244/244 [==============================] - 0s 162us/step - loss: 1.2644 - val_loss: 2.3696\n",
      "Epoch 185/500\n",
      "244/244 [==============================] - 0s 167us/step - loss: 1.2743 - val_loss: 2.3248\n",
      "Epoch 186/500\n",
      "244/244 [==============================] - 0s 204us/step - loss: 1.2728 - val_loss: 2.4488\n",
      "Epoch 187/500\n",
      "244/244 [==============================] - 0s 142us/step - loss: 1.2569 - val_loss: 2.3650\n",
      "Epoch 188/500\n",
      "244/244 [==============================] - 0s 146us/step - loss: 1.2863 - val_loss: 2.4991\n",
      "Epoch 189/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.2900 - val_loss: 2.3555\n",
      "Epoch 190/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.2699 - val_loss: 2.3751\n",
      "Epoch 191/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.2468 - val_loss: 2.3723\n",
      "Epoch 192/500\n",
      "244/244 [==============================] - 0s 198us/step - loss: 1.3253 - val_loss: 2.5280\n",
      "Epoch 193/500\n",
      "244/244 [==============================] - 0s 207us/step - loss: 1.3054 - val_loss: 2.3330\n",
      "Epoch 194/500\n",
      "244/244 [==============================] - 0s 211us/step - loss: 1.2457 - val_loss: 2.4111\n",
      "Epoch 195/500\n",
      "244/244 [==============================] - 0s 241us/step - loss: 1.2576 - val_loss: 2.2949\n",
      "Epoch 196/500\n",
      "244/244 [==============================] - 0s 199us/step - loss: 1.2763 - val_loss: 2.4724\n",
      "Epoch 197/500\n",
      "244/244 [==============================] - 0s 146us/step - loss: 1.2591 - val_loss: 2.3104\n",
      "Epoch 198/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.2664 - val_loss: 2.4817\n",
      "Epoch 199/500\n",
      "244/244 [==============================] - 0s 163us/step - loss: 1.2974 - val_loss: 2.3803\n",
      "Epoch 200/500\n",
      "244/244 [==============================] - 0s 188us/step - loss: 1.2497 - val_loss: 2.3731\n",
      "Epoch 201/500\n",
      "244/244 [==============================] - 0s 258us/step - loss: 1.2463 - val_loss: 2.4295\n",
      "Epoch 202/500\n",
      "244/244 [==============================] - 0s 366us/step - loss: 1.2467 - val_loss: 2.3288\n",
      "Epoch 203/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.2981 - val_loss: 2.4758\n",
      "Epoch 204/500\n",
      "244/244 [==============================] - 0s 178us/step - loss: 1.2857 - val_loss: 2.3276\n",
      "Epoch 205/500\n",
      "244/244 [==============================] - 0s 225us/step - loss: 1.3073 - val_loss: 2.3627\n",
      "Epoch 206/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.3281 - val_loss: 2.5292\n",
      "Epoch 207/500\n",
      "244/244 [==============================] - 0s 187us/step - loss: 1.3220 - val_loss: 2.3954\n",
      "Epoch 208/500\n",
      "244/244 [==============================] - 0s 289us/step - loss: 1.2690 - val_loss: 2.3696\n",
      "Epoch 209/500\n",
      "244/244 [==============================] - 0s 174us/step - loss: 1.2668 - val_loss: 2.3489\n",
      "Epoch 210/500\n",
      "244/244 [==============================] - 0s 178us/step - loss: 1.2772 - val_loss: 2.4084\n",
      "Epoch 211/500\n",
      "244/244 [==============================] - 0s 160us/step - loss: 1.2929 - val_loss: 2.3221\n",
      "Epoch 212/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.2726 - val_loss: 2.4121\n",
      "Epoch 213/500\n",
      "244/244 [==============================] - 0s 148us/step - loss: 1.2347 - val_loss: 2.3389\n",
      "Epoch 214/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2489 - val_loss: 2.3657\n",
      "Epoch 215/500\n",
      "244/244 [==============================] - 0s 171us/step - loss: 1.2387 - val_loss: 2.3157\n",
      "Epoch 216/500\n",
      "244/244 [==============================] - 0s 162us/step - loss: 1.2748 - val_loss: 2.4432\n",
      "Epoch 217/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.2719 - val_loss: 2.4081\n",
      "Epoch 218/500\n",
      "244/244 [==============================] - 0s 166us/step - loss: 1.2691 - val_loss: 2.3083\n",
      "Epoch 219/500\n",
      "244/244 [==============================] - 0s 147us/step - loss: 1.2593 - val_loss: 2.4088\n",
      "Epoch 220/500\n",
      "244/244 [==============================] - 0s 147us/step - loss: 1.2613 - val_loss: 2.3936\n",
      "Epoch 221/500\n",
      "244/244 [==============================] - 0s 145us/step - loss: 1.2598 - val_loss: 2.3201\n",
      "Epoch 222/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.2571 - val_loss: 2.3520\n",
      "Epoch 223/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.2477 - val_loss: 2.3803\n",
      "Epoch 224/500\n",
      "244/244 [==============================] - 0s 163us/step - loss: 1.2404 - val_loss: 2.3681\n",
      "Epoch 225/500\n",
      "244/244 [==============================] - 0s 161us/step - loss: 1.2416 - val_loss: 2.3820\n",
      "Epoch 226/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2453 - val_loss: 2.3826\n",
      "Epoch 227/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2439 - val_loss: 2.3666\n",
      "Epoch 228/500\n",
      "244/244 [==============================] - 0s 172us/step - loss: 1.2487 - val_loss: 2.3579\n",
      "Epoch 229/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2614 - val_loss: 2.3547\n",
      "Epoch 230/500\n",
      "244/244 [==============================] - 0s 171us/step - loss: 1.2496 - val_loss: 2.3503\n",
      "Epoch 231/500\n",
      "244/244 [==============================] - 0s 150us/step - loss: 1.2465 - val_loss: 2.4372\n",
      "Epoch 232/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.2598 - val_loss: 2.3624\n",
      "Epoch 233/500\n",
      "244/244 [==============================] - 0s 139us/step - loss: 1.2313 - val_loss: 2.4447\n",
      "Epoch 234/500\n",
      "244/244 [==============================] - 0s 183us/step - loss: 1.2506 - val_loss: 2.3120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/500\n",
      "244/244 [==============================] - 0s 163us/step - loss: 1.2620 - val_loss: 2.3291\n",
      "Epoch 236/500\n",
      "244/244 [==============================] - 0s 162us/step - loss: 1.2561 - val_loss: 2.3646\n",
      "Epoch 237/500\n",
      "244/244 [==============================] - 0s 150us/step - loss: 1.2329 - val_loss: 2.3957\n",
      "Epoch 238/500\n",
      "244/244 [==============================] - 0s 196us/step - loss: 1.2353 - val_loss: 2.3228\n",
      "Epoch 239/500\n",
      "244/244 [==============================] - 0s 144us/step - loss: 1.2624 - val_loss: 2.4425\n",
      "Epoch 240/500\n",
      "244/244 [==============================] - 0s 144us/step - loss: 1.2405 - val_loss: 2.3826\n",
      "Epoch 241/500\n",
      "244/244 [==============================] - 0s 169us/step - loss: 1.2423 - val_loss: 2.3599\n",
      "Epoch 242/500\n",
      "244/244 [==============================] - 0s 144us/step - loss: 1.2531 - val_loss: 2.3345\n",
      "Epoch 243/500\n",
      "244/244 [==============================] - 0s 159us/step - loss: 1.2336 - val_loss: 2.3285\n",
      "Epoch 244/500\n",
      "244/244 [==============================] - 0s 180us/step - loss: 1.2860 - val_loss: 2.3876\n",
      "Epoch 245/500\n",
      "244/244 [==============================] - 0s 160us/step - loss: 1.2519 - val_loss: 2.4235\n",
      "Epoch 246/500\n",
      "244/244 [==============================] - 0s 206us/step - loss: 1.2555 - val_loss: 2.3417\n",
      "Epoch 247/500\n",
      "244/244 [==============================] - 0s 267us/step - loss: 1.2946 - val_loss: 2.3587\n",
      "Epoch 248/500\n",
      "244/244 [==============================] - 0s 266us/step - loss: 1.2778 - val_loss: 2.5360\n",
      "Epoch 249/500\n",
      "244/244 [==============================] - 0s 257us/step - loss: 1.4184 - val_loss: 2.3258\n",
      "Epoch 250/500\n",
      "244/244 [==============================] - 0s 210us/step - loss: 1.2924 - val_loss: 2.4116\n",
      "Epoch 251/500\n",
      "244/244 [==============================] - 0s 233us/step - loss: 1.2740 - val_loss: 2.3294\n",
      "Epoch 252/500\n",
      "244/244 [==============================] - 0s 221us/step - loss: 1.2842 - val_loss: 2.3454\n",
      "Epoch 253/500\n",
      "244/244 [==============================] - 0s 212us/step - loss: 1.2401 - val_loss: 2.4181\n",
      "Epoch 254/500\n",
      "244/244 [==============================] - 0s 248us/step - loss: 1.2460 - val_loss: 2.3145\n",
      "Epoch 255/500\n",
      "244/244 [==============================] - 0s 260us/step - loss: 1.2321 - val_loss: 2.4031\n",
      "Epoch 256/500\n",
      "244/244 [==============================] - 0s 228us/step - loss: 1.2419 - val_loss: 2.3817\n",
      "Epoch 257/500\n",
      "244/244 [==============================] - 0s 207us/step - loss: 1.2424 - val_loss: 2.3011\n",
      "Epoch 258/500\n",
      "244/244 [==============================] - 0s 214us/step - loss: 1.2293 - val_loss: 2.3814\n",
      "Epoch 259/500\n",
      "244/244 [==============================] - 0s 248us/step - loss: 1.2949 - val_loss: 2.2269\n",
      "Epoch 260/500\n",
      "244/244 [==============================] - 0s 196us/step - loss: 1.2667 - val_loss: 2.4291\n",
      "Epoch 261/500\n",
      "244/244 [==============================] - 0s 191us/step - loss: 1.2443 - val_loss: 2.3587\n",
      "Epoch 262/500\n",
      "244/244 [==============================] - 0s 154us/step - loss: 1.2473 - val_loss: 2.3189\n",
      "Epoch 263/500\n",
      "244/244 [==============================] - 0s 224us/step - loss: 1.2562 - val_loss: 2.4788\n",
      "Epoch 264/500\n",
      "244/244 [==============================] - 0s 164us/step - loss: 1.2594 - val_loss: 2.3558\n",
      "Epoch 265/500\n",
      "244/244 [==============================] - 0s 265us/step - loss: 1.2315 - val_loss: 2.3633\n",
      "Epoch 266/500\n",
      "244/244 [==============================] - 0s 237us/step - loss: 1.2192 - val_loss: 2.2988\n",
      "Epoch 267/500\n",
      "244/244 [==============================] - 0s 251us/step - loss: 1.2438 - val_loss: 2.4113\n",
      "Epoch 268/500\n",
      "244/244 [==============================] - 0s 165us/step - loss: 1.2409 - val_loss: 2.3046\n",
      "Epoch 269/500\n",
      "244/244 [==============================] - 0s 185us/step - loss: 1.2439 - val_loss: 2.3474\n",
      "Epoch 270/500\n",
      "244/244 [==============================] - 0s 208us/step - loss: 1.2278 - val_loss: 2.2964\n",
      "Epoch 271/500\n",
      "244/244 [==============================] - 0s 235us/step - loss: 1.2520 - val_loss: 2.4285\n",
      "Epoch 272/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.2404 - val_loss: 2.3415\n",
      "Epoch 273/500\n",
      "244/244 [==============================] - 0s 161us/step - loss: 1.2263 - val_loss: 2.3064\n",
      "Epoch 274/500\n",
      "244/244 [==============================] - 0s 211us/step - loss: 1.2314 - val_loss: 2.3266\n",
      "Epoch 275/500\n",
      "244/244 [==============================] - 0s 174us/step - loss: 1.2282 - val_loss: 2.3518\n",
      "Epoch 276/500\n",
      "244/244 [==============================] - 0s 204us/step - loss: 1.2263 - val_loss: 2.3279\n",
      "Epoch 277/500\n",
      "244/244 [==============================] - 0s 135us/step - loss: 1.2715 - val_loss: 2.4467\n",
      "Epoch 278/500\n",
      "244/244 [==============================] - 0s 162us/step - loss: 1.2362 - val_loss: 2.3476\n",
      "Epoch 279/500\n",
      "244/244 [==============================] - 0s 171us/step - loss: 1.2242 - val_loss: 2.3120\n",
      "Epoch 280/500\n",
      "244/244 [==============================] - 0s 201us/step - loss: 1.2362 - val_loss: 2.3301\n",
      "Epoch 281/500\n",
      "244/244 [==============================] - 0s 182us/step - loss: 1.2381 - val_loss: 2.3299\n",
      "Epoch 282/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.2218 - val_loss: 2.3519\n",
      "Epoch 283/500\n",
      "244/244 [==============================] - 0s 138us/step - loss: 1.2275 - val_loss: 2.3721\n",
      "Epoch 284/500\n",
      "244/244 [==============================] - 0s 182us/step - loss: 1.2425 - val_loss: 2.3445\n",
      "Epoch 285/500\n",
      "244/244 [==============================] - 0s 184us/step - loss: 1.2165 - val_loss: 2.3053\n",
      "Epoch 286/500\n",
      "244/244 [==============================] - 0s 190us/step - loss: 1.2322 - val_loss: 2.4523\n",
      "Epoch 287/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.2636 - val_loss: 2.3299\n",
      "Epoch 288/500\n",
      "244/244 [==============================] - 0s 212us/step - loss: 1.2594 - val_loss: 2.3280\n",
      "Epoch 289/500\n",
      "244/244 [==============================] - 0s 202us/step - loss: 1.3343 - val_loss: 2.5516\n",
      "Epoch 290/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.3241 - val_loss: 2.2363\n",
      "Epoch 291/500\n",
      "244/244 [==============================] - 0s 219us/step - loss: 1.2586 - val_loss: 2.4589\n",
      "Epoch 292/500\n",
      "244/244 [==============================] - 0s 206us/step - loss: 1.2494 - val_loss: 2.2932\n",
      "Epoch 293/500\n",
      "244/244 [==============================] - 0s 191us/step - loss: 1.2302 - val_loss: 2.3596\n",
      "Epoch 294/500\n",
      "244/244 [==============================] - 0s 147us/step - loss: 1.2362 - val_loss: 2.3959\n",
      "Epoch 295/500\n",
      "244/244 [==============================] - 0s 194us/step - loss: 1.2388 - val_loss: 2.3549\n",
      "Epoch 296/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2167 - val_loss: 2.3172\n",
      "Epoch 297/500\n",
      "244/244 [==============================] - 0s 164us/step - loss: 1.2416 - val_loss: 2.3447\n",
      "Epoch 298/500\n",
      "244/244 [==============================] - 0s 188us/step - loss: 1.2963 - val_loss: 2.2978\n",
      "Epoch 299/500\n",
      "244/244 [==============================] - 0s 189us/step - loss: 1.2422 - val_loss: 2.3786\n",
      "Epoch 300/500\n",
      "244/244 [==============================] - 0s 192us/step - loss: 1.2458 - val_loss: 2.3235\n",
      "Epoch 301/500\n",
      "244/244 [==============================] - 0s 163us/step - loss: 1.2699 - val_loss: 2.3557\n",
      "Epoch 302/500\n",
      "244/244 [==============================] - 0s 183us/step - loss: 1.2254 - val_loss: 2.4391\n",
      "Epoch 303/500\n",
      "244/244 [==============================] - 0s 202us/step - loss: 1.2590 - val_loss: 2.2897\n",
      "Epoch 304/500\n",
      "244/244 [==============================] - 0s 182us/step - loss: 1.3247 - val_loss: 2.5621\n",
      "Epoch 305/500\n",
      "244/244 [==============================] - 0s 168us/step - loss: 1.2866 - val_loss: 2.3494\n",
      "Epoch 306/500\n",
      "244/244 [==============================] - 0s 174us/step - loss: 1.2189 - val_loss: 2.3497\n",
      "Epoch 307/500\n",
      "244/244 [==============================] - 0s 166us/step - loss: 1.2313 - val_loss: 2.4928\n",
      "Epoch 308/500\n",
      "244/244 [==============================] - 0s 189us/step - loss: 1.2236 - val_loss: 2.2491\n",
      "Epoch 309/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.3239 - val_loss: 2.5937\n",
      "Epoch 310/500\n",
      "244/244 [==============================] - 0s 192us/step - loss: 1.2839 - val_loss: 2.2609\n",
      "Epoch 311/500\n",
      "244/244 [==============================] - 0s 189us/step - loss: 1.2891 - val_loss: 2.3882\n",
      "Epoch 312/500\n",
      "244/244 [==============================] - 0s 199us/step - loss: 1.2101 - val_loss: 2.3055\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 0s 179us/step - loss: 1.2957 - val_loss: 2.5013\n",
      "Epoch 314/500\n",
      "244/244 [==============================] - 0s 166us/step - loss: 1.2384 - val_loss: 2.3289\n",
      "Epoch 315/500\n",
      "244/244 [==============================] - 0s 182us/step - loss: 1.2058 - val_loss: 2.4140\n",
      "Epoch 316/500\n",
      "244/244 [==============================] - 0s 260us/step - loss: 1.2183 - val_loss: 2.3277\n",
      "Epoch 317/500\n",
      "244/244 [==============================] - 0s 140us/step - loss: 1.2175 - val_loss: 2.3732\n",
      "Epoch 318/500\n",
      "244/244 [==============================] - 0s 226us/step - loss: 1.2272 - val_loss: 2.3474\n",
      "Epoch 319/500\n",
      "244/244 [==============================] - 0s 161us/step - loss: 1.2154 - val_loss: 2.3052\n",
      "Epoch 320/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.2348 - val_loss: 2.4109\n",
      "Epoch 321/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2280 - val_loss: 2.3376\n",
      "Epoch 322/500\n",
      "244/244 [==============================] - 0s 182us/step - loss: 1.2115 - val_loss: 2.3544\n",
      "Epoch 323/500\n",
      "244/244 [==============================] - 0s 168us/step - loss: 1.2209 - val_loss: 2.3780\n",
      "Epoch 324/500\n",
      "244/244 [==============================] - 0s 148us/step - loss: 1.2352 - val_loss: 2.3176\n",
      "Epoch 325/500\n",
      "244/244 [==============================] - 0s 161us/step - loss: 1.2064 - val_loss: 2.3556\n",
      "Epoch 326/500\n",
      "244/244 [==============================] - 0s 174us/step - loss: 1.2222 - val_loss: 2.4034\n",
      "Epoch 327/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.2155 - val_loss: 2.3453\n",
      "Epoch 328/500\n",
      "244/244 [==============================] - 0s 195us/step - loss: 1.2175 - val_loss: 2.3240\n",
      "Epoch 329/500\n",
      "244/244 [==============================] - 0s 178us/step - loss: 1.2015 - val_loss: 2.4727\n",
      "Epoch 330/500\n",
      "244/244 [==============================] - 0s 154us/step - loss: 1.2379 - val_loss: 2.3259\n",
      "Epoch 331/500\n",
      "244/244 [==============================] - 0s 169us/step - loss: 1.2269 - val_loss: 2.3153\n",
      "Epoch 332/500\n",
      "244/244 [==============================] - 0s 198us/step - loss: 1.2392 - val_loss: 2.4951\n",
      "Epoch 333/500\n",
      "244/244 [==============================] - 0s 188us/step - loss: 1.2414 - val_loss: 2.3116\n",
      "Epoch 334/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.2567 - val_loss: 2.3307\n",
      "Epoch 335/500\n",
      "244/244 [==============================] - ETA: 0s - loss: 1.102 - 0s 205us/step - loss: 1.2113 - val_loss: 2.3349\n",
      "Epoch 336/500\n",
      "244/244 [==============================] - 0s 159us/step - loss: 1.2233 - val_loss: 2.2991\n",
      "Epoch 337/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2735 - val_loss: 2.5039\n",
      "Epoch 338/500\n",
      "244/244 [==============================] - 0s 147us/step - loss: 1.2578 - val_loss: 2.2339\n",
      "Epoch 339/500\n",
      "244/244 [==============================] - 0s 143us/step - loss: 1.2941 - val_loss: 2.5094\n",
      "Epoch 340/500\n",
      "244/244 [==============================] - 0s 182us/step - loss: 1.2556 - val_loss: 2.3312\n",
      "Epoch 341/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2432 - val_loss: 2.3132\n",
      "Epoch 342/500\n",
      "244/244 [==============================] - 0s 196us/step - loss: 1.2267 - val_loss: 2.4707\n",
      "Epoch 343/500\n",
      "244/244 [==============================] - 0s 176us/step - loss: 1.2197 - val_loss: 2.2700\n",
      "Epoch 344/500\n",
      "244/244 [==============================] - 0s 191us/step - loss: 1.2591 - val_loss: 2.4712\n",
      "Epoch 345/500\n",
      "244/244 [==============================] - 0s 195us/step - loss: 1.2480 - val_loss: 2.3120\n",
      "Epoch 346/500\n",
      "244/244 [==============================] - 0s 188us/step - loss: 1.2397 - val_loss: 2.4216\n",
      "Epoch 347/500\n",
      "244/244 [==============================] - 0s 169us/step - loss: 1.2124 - val_loss: 2.3631\n",
      "Epoch 348/500\n",
      "244/244 [==============================] - 0s 168us/step - loss: 1.2050 - val_loss: 2.3537\n",
      "Epoch 349/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.2168 - val_loss: 2.3866\n",
      "Epoch 350/500\n",
      "244/244 [==============================] - 0s 167us/step - loss: 1.2179 - val_loss: 2.3634\n",
      "Epoch 351/500\n",
      "244/244 [==============================] - 0s 159us/step - loss: 1.2143 - val_loss: 2.3198\n",
      "Epoch 352/500\n",
      "244/244 [==============================] - 0s 172us/step - loss: 1.2002 - val_loss: 2.3690\n",
      "Epoch 353/500\n",
      "244/244 [==============================] - 0s 154us/step - loss: 1.2080 - val_loss: 2.3644\n",
      "Epoch 354/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2160 - val_loss: 2.3617\n",
      "Epoch 355/500\n",
      "244/244 [==============================] - 0s 140us/step - loss: 1.2103 - val_loss: 2.2799\n",
      "Epoch 356/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2543 - val_loss: 2.3609\n",
      "Epoch 357/500\n",
      "244/244 [==============================] - 0s 168us/step - loss: 1.2093 - val_loss: 2.3172\n",
      "Epoch 358/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.2060 - val_loss: 2.3591\n",
      "Epoch 359/500\n",
      "244/244 [==============================] - 0s 144us/step - loss: 1.1956 - val_loss: 2.3226\n",
      "Epoch 360/500\n",
      "244/244 [==============================] - 0s 167us/step - loss: 1.2220 - val_loss: 2.5095\n",
      "Epoch 361/500\n",
      "244/244 [==============================] - 0s 181us/step - loss: 1.2175 - val_loss: 2.2943\n",
      "Epoch 362/500\n",
      "244/244 [==============================] - 0s 164us/step - loss: 1.2384 - val_loss: 2.3978\n",
      "Epoch 363/500\n",
      "244/244 [==============================] - 0s 154us/step - loss: 1.2123 - val_loss: 2.3249\n",
      "Epoch 364/500\n",
      "244/244 [==============================] - 0s 191us/step - loss: 1.2347 - val_loss: 2.4825\n",
      "Epoch 365/500\n",
      "244/244 [==============================] - 0s 154us/step - loss: 1.2334 - val_loss: 2.2913\n",
      "Epoch 366/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.2411 - val_loss: 2.5190\n",
      "Epoch 367/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.2710 - val_loss: 2.3056\n",
      "Epoch 368/500\n",
      "244/244 [==============================] - 0s 168us/step - loss: 1.2323 - val_loss: 2.3932\n",
      "Epoch 369/500\n",
      "244/244 [==============================] - 0s 160us/step - loss: 1.2086 - val_loss: 2.3417\n",
      "Epoch 370/500\n",
      "244/244 [==============================] - 0s 145us/step - loss: 1.1957 - val_loss: 2.3802\n",
      "Epoch 371/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.1956 - val_loss: 2.3020\n",
      "Epoch 372/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2043 - val_loss: 2.4464\n",
      "Epoch 373/500\n",
      "244/244 [==============================] - 0s 161us/step - loss: 1.2229 - val_loss: 2.2934\n",
      "Epoch 374/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2059 - val_loss: 2.4021\n",
      "Epoch 375/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.2135 - val_loss: 2.3346\n",
      "Epoch 376/500\n",
      "244/244 [==============================] - 0s 147us/step - loss: 1.2333 - val_loss: 2.3536\n",
      "Epoch 377/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.2106 - val_loss: 2.3737\n",
      "Epoch 378/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2090 - val_loss: 2.3195\n",
      "Epoch 379/500\n",
      "244/244 [==============================] - 0s 165us/step - loss: 1.2005 - val_loss: 2.4167\n",
      "Epoch 380/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2078 - val_loss: 2.3192\n",
      "Epoch 381/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2021 - val_loss: 2.4208\n",
      "Epoch 382/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.2241 - val_loss: 2.2833\n",
      "Epoch 383/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.2172 - val_loss: 2.4212\n",
      "Epoch 384/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2621 - val_loss: 2.3300\n",
      "Epoch 385/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.2187 - val_loss: 2.3374\n",
      "Epoch 386/500\n",
      "244/244 [==============================] - 0s 148us/step - loss: 1.2080 - val_loss: 2.3147\n",
      "Epoch 387/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2135 - val_loss: 2.3306\n",
      "Epoch 388/500\n",
      "244/244 [==============================] - 0s 181us/step - loss: 1.2134 - val_loss: 2.4505\n",
      "Epoch 389/500\n",
      "244/244 [==============================] - 0s 149us/step - loss: 1.2261 - val_loss: 2.3065\n",
      "Epoch 390/500\n",
      "244/244 [==============================] - 0s 147us/step - loss: 1.2124 - val_loss: 2.3718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.2145 - val_loss: 2.3303\n",
      "Epoch 392/500\n",
      "244/244 [==============================] - 0s 174us/step - loss: 1.2123 - val_loss: 2.3510\n",
      "Epoch 393/500\n",
      "244/244 [==============================] - 0s 226us/step - loss: 1.1962 - val_loss: 2.3399\n",
      "Epoch 394/500\n",
      "244/244 [==============================] - 0s 271us/step - loss: 1.2095 - val_loss: 2.3127\n",
      "Epoch 395/500\n",
      "244/244 [==============================] - 0s 215us/step - loss: 1.2145 - val_loss: 2.3606\n",
      "Epoch 396/500\n",
      "244/244 [==============================] - 0s 228us/step - loss: 1.1977 - val_loss: 2.3476\n",
      "Epoch 397/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2065 - val_loss: 2.3345\n",
      "Epoch 398/500\n",
      "244/244 [==============================] - ETA: 0s - loss: 1.188 - 0s 182us/step - loss: 1.2160 - val_loss: 2.3778\n",
      "Epoch 399/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.2173 - val_loss: 2.4512\n",
      "Epoch 400/500\n",
      "244/244 [==============================] - 0s 229us/step - loss: 1.2262 - val_loss: 2.4580\n",
      "Epoch 401/500\n",
      "244/244 [==============================] - 0s 169us/step - loss: 1.2620 - val_loss: 2.3054\n",
      "Epoch 402/500\n",
      "244/244 [==============================] - 0s 206us/step - loss: 1.2314 - val_loss: 2.3991\n",
      "Epoch 403/500\n",
      "244/244 [==============================] - 0s 182us/step - loss: 1.2068 - val_loss: 2.3462\n",
      "Epoch 404/500\n",
      "244/244 [==============================] - 0s 217us/step - loss: 1.2071 - val_loss: 2.3238\n",
      "Epoch 405/500\n",
      "244/244 [==============================] - 0s 204us/step - loss: 1.2053 - val_loss: 2.3954\n",
      "Epoch 406/500\n",
      "244/244 [==============================] - 0s 160us/step - loss: 1.2200 - val_loss: 2.3604\n",
      "Epoch 407/500\n",
      "244/244 [==============================] - 0s 155us/step - loss: 1.2042 - val_loss: 2.3467\n",
      "Epoch 408/500\n",
      "244/244 [==============================] - 0s 202us/step - loss: 1.2249 - val_loss: 2.3697\n",
      "Epoch 409/500\n",
      "244/244 [==============================] - 0s 161us/step - loss: 1.2299 - val_loss: 2.4496\n",
      "Epoch 410/500\n",
      "244/244 [==============================] - 0s 126us/step - loss: 1.2420 - val_loss: 2.3240\n",
      "Epoch 411/500\n",
      "244/244 [==============================] - 0s 255us/step - loss: 1.2011 - val_loss: 2.3269\n",
      "Epoch 412/500\n",
      "244/244 [==============================] - 0s 177us/step - loss: 1.1933 - val_loss: 2.4107\n",
      "Epoch 413/500\n",
      "244/244 [==============================] - 0s 225us/step - loss: 1.2370 - val_loss: 2.3129\n",
      "Epoch 414/500\n",
      "244/244 [==============================] - 0s 196us/step - loss: 1.2007 - val_loss: 2.4429\n",
      "Epoch 415/500\n",
      "244/244 [==============================] - 0s 229us/step - loss: 1.2775 - val_loss: 2.3586\n",
      "Epoch 416/500\n",
      "244/244 [==============================] - 0s 228us/step - loss: 1.2812 - val_loss: 2.2771\n",
      "Epoch 417/500\n",
      "244/244 [==============================] - 0s 146us/step - loss: 1.3320 - val_loss: 2.4738\n",
      "Epoch 418/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.2308 - val_loss: 2.3264\n",
      "Epoch 419/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.2202 - val_loss: 2.3386\n",
      "Epoch 420/500\n",
      "244/244 [==============================] - 0s 181us/step - loss: 1.2061 - val_loss: 2.4244\n",
      "Epoch 421/500\n",
      "244/244 [==============================] - 0s 185us/step - loss: 1.1984 - val_loss: 2.3241\n",
      "Epoch 422/500\n",
      "244/244 [==============================] - 0s 167us/step - loss: 1.1954 - val_loss: 2.4140\n",
      "Epoch 423/500\n",
      "244/244 [==============================] - 0s 165us/step - loss: 1.2244 - val_loss: 2.2929\n",
      "Epoch 424/500\n",
      "244/244 [==============================] - 0s 179us/step - loss: 1.2419 - val_loss: 2.4026\n",
      "Epoch 425/500\n",
      "244/244 [==============================] - 0s 191us/step - loss: 1.2096 - val_loss: 2.3666\n",
      "Epoch 426/500\n",
      "244/244 [==============================] - 0s 216us/step - loss: 1.2113 - val_loss: 2.3149\n",
      "Epoch 427/500\n",
      "244/244 [==============================] - 0s 241us/step - loss: 1.2165 - val_loss: 2.3386\n",
      "Epoch 428/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.2029 - val_loss: 2.3427\n",
      "Epoch 429/500\n",
      "244/244 [==============================] - 0s 198us/step - loss: 1.2060 - val_loss: 2.3908\n",
      "Epoch 430/500\n",
      "244/244 [==============================] - 0s 180us/step - loss: 1.2076 - val_loss: 2.3233\n",
      "Epoch 431/500\n",
      "244/244 [==============================] - 0s 145us/step - loss: 1.1994 - val_loss: 2.3389\n",
      "Epoch 432/500\n",
      "244/244 [==============================] - 0s 167us/step - loss: 1.2032 - val_loss: 2.3496\n",
      "Epoch 433/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2010 - val_loss: 2.3795\n",
      "Epoch 434/500\n",
      "244/244 [==============================] - 0s 169us/step - loss: 1.1960 - val_loss: 2.3809\n",
      "Epoch 435/500\n",
      "244/244 [==============================] - 0s 142us/step - loss: 1.2017 - val_loss: 2.3021\n",
      "Epoch 436/500\n",
      "244/244 [==============================] - 0s 196us/step - loss: 1.2192 - val_loss: 2.4013\n",
      "Epoch 437/500\n",
      "244/244 [==============================] - 0s 148us/step - loss: 1.2122 - val_loss: 2.3430\n",
      "Epoch 438/500\n",
      "244/244 [==============================] - 0s 160us/step - loss: 1.1907 - val_loss: 2.3611\n",
      "Epoch 439/500\n",
      "244/244 [==============================] - 0s 164us/step - loss: 1.1977 - val_loss: 2.3248\n",
      "Epoch 440/500\n",
      "244/244 [==============================] - 0s 175us/step - loss: 1.2071 - val_loss: 2.3268\n",
      "Epoch 441/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2028 - val_loss: 2.3777\n",
      "Epoch 442/500\n",
      "244/244 [==============================] - 0s 183us/step - loss: 1.1934 - val_loss: 2.3675\n",
      "Epoch 443/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.1925 - val_loss: 2.3855\n",
      "Epoch 444/500\n",
      "244/244 [==============================] - 0s 186us/step - loss: 1.2006 - val_loss: 2.3996\n",
      "Epoch 445/500\n",
      "244/244 [==============================] - 0s 165us/step - loss: 1.2163 - val_loss: 2.2918\n",
      "Epoch 446/500\n",
      "244/244 [==============================] - 0s 171us/step - loss: 1.2835 - val_loss: 2.4960\n",
      "Epoch 447/500\n",
      "244/244 [==============================] - 0s 154us/step - loss: 1.2343 - val_loss: 2.3081\n",
      "Epoch 448/500\n",
      "244/244 [==============================] - 0s 163us/step - loss: 1.2008 - val_loss: 2.5041\n",
      "Epoch 449/500\n",
      "244/244 [==============================] - 0s 146us/step - loss: 1.2348 - val_loss: 2.2900\n",
      "Epoch 450/500\n",
      "244/244 [==============================] - 0s 143us/step - loss: 1.2602 - val_loss: 2.4667\n",
      "Epoch 451/500\n",
      "244/244 [==============================] - 0s 130us/step - loss: 1.2149 - val_loss: 2.3348\n",
      "Epoch 452/500\n",
      "244/244 [==============================] - 0s 160us/step - loss: 1.2748 - val_loss: 2.2884\n",
      "Epoch 453/500\n",
      "244/244 [==============================] - 0s 153us/step - loss: 1.2766 - val_loss: 2.3674\n",
      "Epoch 454/500\n",
      "244/244 [==============================] - 0s 133us/step - loss: 1.1968 - val_loss: 2.3455\n",
      "Epoch 455/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2042 - val_loss: 2.4766\n",
      "Epoch 456/500\n",
      "244/244 [==============================] - 0s 171us/step - loss: 1.2504 - val_loss: 2.3965\n",
      "Epoch 457/500\n",
      "244/244 [==============================] - 0s 128us/step - loss: 1.2217 - val_loss: 2.3031\n",
      "Epoch 458/500\n",
      "244/244 [==============================] - 0s 150us/step - loss: 1.2484 - val_loss: 2.3851\n",
      "Epoch 459/500\n",
      "244/244 [==============================] - 0s 174us/step - loss: 1.1870 - val_loss: 2.2806\n",
      "Epoch 460/500\n",
      "244/244 [==============================] - 0s 149us/step - loss: 1.2379 - val_loss: 2.4832\n",
      "Epoch 461/500\n",
      "244/244 [==============================] - 0s 147us/step - loss: 1.2580 - val_loss: 2.3695\n",
      "Epoch 462/500\n",
      "244/244 [==============================] - 0s 143us/step - loss: 1.2509 - val_loss: 2.3278\n",
      "Epoch 463/500\n",
      "244/244 [==============================] - 0s 152us/step - loss: 1.2013 - val_loss: 2.3515\n",
      "Epoch 464/500\n",
      "244/244 [==============================] - 0s 163us/step - loss: 1.1916 - val_loss: 2.3558\n",
      "Epoch 465/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.1965 - val_loss: 2.3354\n",
      "Epoch 466/500\n",
      "244/244 [==============================] - 0s 149us/step - loss: 1.2174 - val_loss: 2.4739\n",
      "Epoch 467/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2426 - val_loss: 2.3245\n",
      "Epoch 468/500\n",
      "244/244 [==============================] - 0s 148us/step - loss: 1.2282 - val_loss: 2.3334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469/500\n",
      "244/244 [==============================] - 0s 170us/step - loss: 1.2283 - val_loss: 2.3881\n",
      "Epoch 470/500\n",
      "244/244 [==============================] - 0s 207us/step - loss: 1.2114 - val_loss: 2.3794\n",
      "Epoch 471/500\n",
      "244/244 [==============================] - 0s 137us/step - loss: 1.2270 - val_loss: 2.3070\n",
      "Epoch 472/500\n",
      "244/244 [==============================] - 0s 157us/step - loss: 1.2306 - val_loss: 2.5042\n",
      "Epoch 473/500\n",
      "244/244 [==============================] - 0s 132us/step - loss: 1.2575 - val_loss: 2.3041\n",
      "Epoch 474/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.2189 - val_loss: 2.3709\n",
      "Epoch 475/500\n",
      "244/244 [==============================] - 0s 179us/step - loss: 1.2039 - val_loss: 2.3499\n",
      "Epoch 476/500\n",
      "244/244 [==============================] - 0s 156us/step - loss: 1.2063 - val_loss: 2.3709\n",
      "Epoch 477/500\n",
      "244/244 [==============================] - 0s 145us/step - loss: 1.2187 - val_loss: 2.4061\n",
      "Epoch 478/500\n",
      "244/244 [==============================] - 0s 143us/step - loss: 1.1967 - val_loss: 2.3600\n",
      "Epoch 479/500\n",
      "244/244 [==============================] - 0s 185us/step - loss: 1.1875 - val_loss: 2.4006\n",
      "Epoch 480/500\n",
      "244/244 [==============================] - 0s 182us/step - loss: 1.1970 - val_loss: 2.3840\n",
      "Epoch 481/500\n",
      "244/244 [==============================] - 0s 162us/step - loss: 1.2222 - val_loss: 2.4292\n",
      "Epoch 482/500\n",
      "244/244 [==============================] - 0s 159us/step - loss: 1.2064 - val_loss: 2.3362\n",
      "Epoch 483/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2222 - val_loss: 2.3932\n",
      "Epoch 484/500\n",
      "244/244 [==============================] - 0s 161us/step - loss: 1.1872 - val_loss: 2.3108\n",
      "Epoch 485/500\n",
      "244/244 [==============================] - 0s 162us/step - loss: 1.2152 - val_loss: 2.3807\n",
      "Epoch 486/500\n",
      "244/244 [==============================] - 0s 143us/step - loss: 1.2012 - val_loss: 2.3902\n",
      "Epoch 487/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.2125 - val_loss: 2.3145\n",
      "Epoch 488/500\n",
      "244/244 [==============================] - 0s 186us/step - loss: 1.2217 - val_loss: 2.4151\n",
      "Epoch 489/500\n",
      "244/244 [==============================] - 0s 169us/step - loss: 1.1994 - val_loss: 2.3358\n",
      "Epoch 490/500\n",
      "244/244 [==============================] - 0s 158us/step - loss: 1.1986 - val_loss: 2.3454\n",
      "Epoch 491/500\n",
      "244/244 [==============================] - 0s 171us/step - loss: 1.2022 - val_loss: 2.3471\n",
      "Epoch 492/500\n",
      "244/244 [==============================] - 0s 226us/step - loss: 1.2111 - val_loss: 2.4355\n",
      "Epoch 493/500\n",
      "244/244 [==============================] - 0s 151us/step - loss: 1.2040 - val_loss: 2.3331\n",
      "Epoch 494/500\n",
      "244/244 [==============================] - 0s 173us/step - loss: 1.2090 - val_loss: 2.3327\n",
      "Epoch 495/500\n",
      "244/244 [==============================] - 0s 159us/step - loss: 1.2352 - val_loss: 2.4500\n",
      "Epoch 496/500\n",
      "244/244 [==============================] - 0s 181us/step - loss: 1.1964 - val_loss: 2.2602\n",
      "Epoch 497/500\n",
      "244/244 [==============================] - 0s 178us/step - loss: 1.2901 - val_loss: 2.4283\n",
      "Epoch 498/500\n",
      "244/244 [==============================] - 0s 178us/step - loss: 1.1952 - val_loss: 2.3462\n",
      "Epoch 499/500\n",
      "244/244 [==============================] - 0s 154us/step - loss: 1.1916 - val_loss: 2.3704\n",
      "Epoch 500/500\n",
      "244/244 [==============================] - 0s 166us/step - loss: 1.1941 - val_loss: 2.3770\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(125, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(75, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(25, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(5, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=root_mean_squared_error, optimizer='adam')\n",
    "history = model.fit(X, y, batch_size=32, epochs=500, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0738949314240487\n"
     ]
    }
   ],
   "source": [
    "print min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Readings on Naive Methods**\n",
    "\n",
    "MLP: 2.073\n",
    "\n",
    "SVM: 1.957\n",
    "\n",
    "Extra Trees: 1.927\n",
    "\n",
    "Random Forest: 1.874\n",
    "\n",
    "Gradient Tree Boosting: 1.869\n",
    "\n",
    "Kernel Ridge: 1.839\n",
    "\n",
    "AdaBoost: 1.820"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
