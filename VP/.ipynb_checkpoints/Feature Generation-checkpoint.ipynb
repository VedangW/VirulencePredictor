{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the features from the Influenza virus strains, the following steps are taken:\n",
    "1. Extract the specific segments from each of the different viruses and store them into separate segment files.\n",
    "2. Align each segment file using Clustal Omega and store back as a fasta file.\n",
    "3. Preprocess each alignment file by replacing any unknown symbols except '-' with an 'X'.\n",
    "4. For each alignment file, encode the sequences from the file according to AAIndex (Relative frequency of occurence) using Quantiprot.\n",
    "5. Gather the encoded sequences from each alignment file in the form of a dictionary where the keys are the identifiers in the file or the Influenza strains.\n",
    "6. Recombine the sequences corresponding to each segment for each Influenza strain.\n",
    "7. For each virus strain, concatenate its sequences to form a large embedding (~10e4).\n",
    "8. Use an autoencoder to reduce the dimensionality to around 100 features by training all samples on it once and then using the encoder to encode these sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vedang/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import click\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from quantiprot.utils.mapping import simplify\n",
    "from quantiprot.utils.io import load_fasta_file\n",
    "from quantiprot.metrics.aaindex import get_aaindex_file\n",
    "from quantiprot.utils.feature import Feature, FeatureSet\n",
    "from quantiprot.utils.sequence import SequenceSet, subset, columns\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences_from_file(fname, dirname):\n",
    "    \"\"\" Function to get the encodings for all\n",
    "        sequences from a fasta file which is aligned.\n",
    "\n",
    "        This encoding method uses AAIndex to get the\n",
    "        encodings. The amino acid index used for this\n",
    "        currently is 'JOND920101' - Relative frequency \n",
    "        of occurrence (Jones et al., 1992).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname: str\n",
    "            Name of the file from which to read sequences.\n",
    "\n",
    "        dirname: str\n",
    "            Name of the file in which 'fname' is present.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        encoded: list\n",
    "            A list of the encoded sequences.\n",
    "\n",
    "        log: list\n",
    "            A log containing any errors encountered.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the path to the file\n",
    "    fpath = dirname + '/' + fname\n",
    "\n",
    "    # Read file as a set of Quantiprot sequences\n",
    "    sequences = load_fasta_file(fpath)\n",
    "\n",
    "    # Create a Feature object\n",
    "    aa2freq_map = get_aaindex_file(\"JOND920101\")\n",
    "    aa2freq_map.mapping['-'] = 0.0\n",
    "    aa2freq_map.mapping['X'] = 0.0\n",
    "    freq_feat = Feature(aa2freq_map)\n",
    "\n",
    "    # Encode sequences using relative frequency\n",
    "    encoded, log = [], []\n",
    "    for seq in sequences:\n",
    "        try:\n",
    "            f = freq_feat(seq)\n",
    "            encoded.append(f.data)\n",
    "        except:\n",
    "            # For logging errors\n",
    "            e, message, _tb = sys.exc_info()\n",
    "            log.append(fname + ': line ' + str(e) + ': ' + str(message))\n",
    "            continue\n",
    "\n",
    "    return encoded, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'aligned'\n",
    "files = os.listdir(dirname)\n",
    "\n",
    "# Encode all sequences in all files\n",
    "# in the given directory\n",
    "full_log, enc_seqs = [], []\n",
    "for fname in files:\n",
    "    enc, log = encode_sequences_from_file(fname, dirname)\n",
    "    enc_seqs.append(enc)\n",
    "    full_log += log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "# Check length of the full log. Without preprocessing it shouldn't be zero.\n",
    "print len(full_log)\n",
    "print np.shape(enc_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding Sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of sequence in each segment is stored in sizes\n",
    "sizes = []\n",
    "for seq in enc_seqs:\n",
    "    sizes.append(np.shape(seq)[1])\n",
    "    \n",
    "# The maximum size\n",
    "maximum_size = max(sizes)\n",
    "\n",
    "# Pad the sequences according to the maximum size.\n",
    "for i in range(len(enc_seqs)):\n",
    "    enc_seqs[i] = pad_sequences(enc_seqs[i], maxlen=maximum_size, dtype='float32', padding='post', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[750, 139, 305, 253, 505, 90, 772, 252, 100, 128, 614, 566, 759]\n",
      "772\n",
      "(215, 772)\n",
      "(215, 772)\n",
      "(214, 772)\n",
      "(215, 772)\n",
      "(214, 772)\n",
      "(171, 772)\n",
      "(215, 772)\n",
      "(207, 772)\n",
      "(7, 772)\n",
      "(215, 772)\n",
      "(213, 772)\n",
      "(214, 772)\n",
      "(215, 772)\n"
     ]
    }
   ],
   "source": [
    "print sizes\n",
    "print maximum_size\n",
    "\n",
    "for seq in enc_seqs:\n",
    "    print np.shape(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using an autoencoder**\n",
    "\n",
    "This is just for testing purpose, without recombination of the sequences according to virus name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2530, 772)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate(enc_seqs, axis=0)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Dimension of the hidden layer\n",
    "encoding_dim = 32\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(772,))\n",
    "\n",
    "# Encoder layers\n",
    "encoded1 = Dense(128, activation='relu')(input_layer)\n",
    "encoded2 = Dense(64, activation='relu')(encoded1)\n",
    "latent = Dense(encoding_dim, activation='relu')(encoded2)\n",
    "\n",
    "# Decoder layers\n",
    "decoded1 = Dense(64, activation='relu')(latent)\n",
    "decoded2 = Dense(128, activation='relu')(decoded1)\n",
    "reconstruction = Dense(772, activation='sigmoid')(decoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the whole autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=reconstruction)\n",
    "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 772)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               98944     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 772)               99588     \n",
      "=================================================================\n",
      "Total params: 219,300\n",
      "Trainable params: 219,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of autoencoder\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vedang/.local/lib/python2.7/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2024 samples, validate on 506 samples\n",
      "Epoch 1/5\n",
      "2024/2024 [==============================] - 2s 1ms/step - loss: 0.6795 - val_loss: 0.6477\n",
      "Epoch 2/5\n",
      "2024/2024 [==============================] - 1s 340us/step - loss: 0.2440 - val_loss: 0.1879\n",
      "Epoch 3/5\n",
      "2024/2024 [==============================] - 1s 411us/step - loss: 0.1162 - val_loss: 0.1859\n",
      "Epoch 4/5\n",
      "2024/2024 [==============================] - 1s 442us/step - loss: 0.1124 - val_loss: 0.1860\n",
      "Epoch 5/5\n",
      "2024/2024 [==============================] - 1s 326us/step - loss: 0.1115 - val_loss: 0.1865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc1b05e5d50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the autoencoder with the data\n",
    "autoencoder.fit(X, X, nb_epoch=5, batch_size=32, shuffle=False, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder model\n",
    "encoder = Model(inputs=input_layer, outputs=latent)\n",
    "encoded_input = Input(shape =(encoding_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the training set on it again\n",
    "X_enc = encoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2530, 32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of the encoded model\n",
    "np.shape(X_enc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
